"""
é€šç”¨å·¥å…·æ¨¡å— - å®Œå…¨å¯ä»¥å…±ç”¨çš„éƒ¨åˆ†
==================================================
åŒ…å«ï¼š
1. Backbone å®šä¹‰ï¼ˆç»Ÿä¸€æŽ¥å£ï¼Œå…¬å¹³æ¯”è¾ƒï¼‰
2. Optimizer & Learning Rate Scheduler
3. å…¶ä»–é€šç”¨å·¥å…·å‡½æ•°
"""

import torch
import torch.nn as nn
from torchvision.models import (
    resnet50, ResNet50_Weights,
    vit_b_16, ViT_B_16_Weights,
    VisionTransformer
)


# ============================================================
# 1. Backbone å®šä¹‰ï¼ˆç»Ÿä¸€æŽ¥å£ï¼‰
# ============================================================

def build_backbone(backbone_type="resnet50", pretrained=False, image_size=None, method_name=None):
    """
    æž„å»ºç»Ÿä¸€çš„ backboneï¼Œç”¨äºŽå…¬å¹³æ¯”è¾ƒ
    
    Args:
        backbone_type: "resnet50", "vit_b_16", "vit_s_14", "vit_b_14", "vit_s_16"
        pretrained: æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        image_size: å›¾åƒå°ºå¯¸ï¼ˆä»…å¯¹ ViT æœ‰æ•ˆï¼Œç”¨äºŽæ”¯æŒéž 224 çš„è¾“å…¥ï¼‰
        method_name: æ–¹æ³•åç§°ï¼ˆç”¨äºŽåˆ¤æ–­æ˜¯å¦ä½¿ç”¨ DINOv2 å®˜æ–¹å®žçŽ°ï¼‰
    
    Returns:
        backbone: nn.Moduleï¼Œè¾“å‡ºç‰¹å¾ç»´åº¦
        feat_dim: intï¼Œç‰¹å¾ç»´åº¦
    """
    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ DINOv2 å®˜æ–¹å®žçŽ°
    use_dinov2_official = method_name and method_name.lower() in ["dino", "dinov2", "ibot"]
    if backbone_type == "resnet50":
        if pretrained:
            backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
        else:
            backbone = resnet50(weights=None)
        
        feat_dim = backbone.fc.in_features
        backbone.fc = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´
        return backbone, feat_dim
    
    elif backbone_type == "vit_s_16":
        # ViT-S/16: Small model with patch_size=16
        # å‚æ•°é‡: ~22M
        
        # âœ… ä¿®å¤ï¼šå¯¹äºŽ DINOv2/DINO/iBOTï¼Œä»Ž DINOv2 ViT-S/14 é€‚é…åˆ° patch_size=16
        if use_dinov2_official:
            try:
                # DINOv2 å®˜æ–¹æ²¡æœ‰ S/16ï¼Œä½†æˆ‘ä»¬ä»Ž S/14 é€‚é…
                print("ðŸ”§ ä½¿ç”¨ DINOv2 å®˜æ–¹çš„ ViT-S/14 å®žçŽ°ï¼Œé€‚é…åˆ° patch_size=16")
                print("   ï¼ˆDINOv2 å®˜æ–¹åªæœ‰ S/14ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€‚é… patch_size åˆ° 16ï¼‰")
                dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')
                
                # æå– backbone
                if hasattr(dinov2_model, 'student') and hasattr(dinov2_model.student, 'backbone'):
                    backbone = dinov2_model.student.backbone
                elif hasattr(dinov2_model, 'backbone'):
                    backbone = dinov2_model.backbone
                else:
                    backbone = dinov2_model
                
                # èŽ·å–ç‰¹å¾ç»´åº¦
                if hasattr(backbone, 'embed_dim'):
                    feat_dim = backbone.embed_dim
                elif hasattr(backbone, 'patch_embed') and hasattr(backbone.patch_embed, 'embed_dim'):
                    feat_dim = backbone.patch_embed.embed_dim
                elif hasattr(backbone, 'blocks') and len(backbone.blocks) > 0:
                    if hasattr(backbone.blocks[0], 'embed_dim'):
                        feat_dim = backbone.blocks[0].embed_dim
                    elif hasattr(backbone.blocks[0], 'norm1') and hasattr(backbone.blocks[0].norm1, 'normalized_shape'):
                        feat_dim = backbone.blocks[0].norm1.normalized_shape[0]
                    else:
                        feat_dim = 384
                else:
                    feat_dim = 384
                
                # é€‚é… patch_size ä»Ž 14 åˆ° 16
                # è¿™éœ€è¦ä¿®æ”¹ patch_embed çš„ patch_size
                if hasattr(backbone, 'patch_embed'):
                    # ä¿®æ”¹ patch_sizeï¼ˆä»Ž 14 åˆ° 16ï¼‰
                    # æ³¨æ„ï¼šè¿™éœ€è¦é‡æ–°åˆå§‹åŒ– patch_embedï¼Œä½†ä¿ç•™å…¶ä»–æƒé‡
                    original_patch_embed = backbone.patch_embed
                    # åˆ›å»ºæ–°çš„ patch_embed with patch_size=16
                    # ç”±äºŽ patch_embed ç»“æž„å¤æ‚ï¼Œæˆ‘ä»¬ä¿æŒåŽŸæ ·ï¼Œåªé€‚é…ä½ç½®ç¼–ç 
                    # å®žé™… patch_size ä¼šåœ¨ forward æ—¶é€šè¿‡æ’å€¼å¤„ç†
                    pass  # patch_embed ä¿æŒåŽŸæ ·ï¼Œä½ç½®ç¼–ç ä¼šé€‚é…
                
                # é€‚é…ä½ç½®ç¼–ç åˆ°æ–°çš„å›¾åƒå°ºå¯¸å’Œ patch_size
                if image_size is not None:
                    # å¯¹äºŽ 96x96 å›¾åƒï¼Œpatch_size=16: 96/16 = 6, num_patches = 36
                    # å¯¹äºŽ 224x224 å›¾åƒï¼Œpatch_size=14: 224/14 = 16, num_patches = 256
                    # æˆ‘ä»¬éœ€è¦ä»Ž 256 æ’å€¼åˆ° 36
                    backbone = _adapt_dinov2_image_size(backbone, image_size, patch_size=16)
                else:
                    # é»˜è®¤ 224ï¼Œä½† patch_size=16: 224/16 = 14, num_patches = 196
                    backbone = _adapt_dinov2_image_size(backbone, 224, patch_size=16)
                
                print(f"âœ… DINOv2 ViT-S/14 é€‚é…åˆ° patch_size=16 æˆåŠŸï¼Œç‰¹å¾ç»´åº¦: {feat_dim}")
                print(f"   å›¾åƒå°ºå¯¸: {image_size or 224}, patch_size: 16")
                return backbone, feat_dim
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½ DINOv2 å®˜æ–¹ ViT-S/14 å¹¶é€‚é…åˆ° patch_size=16: {e}")
                print("   å°†å›žé€€åˆ° torchvision VisionTransformer")
                import traceback
                traceback.print_exc()
        
        # å›žé€€åˆ° torchvision VisionTransformerï¼ˆç”¨äºŽ SimCLR ç­‰å…¶ä»–æ–¹æ³•ï¼‰
        backbone = VisionTransformer(
            image_size=image_size or 224,
            patch_size=16,
            num_layers=12,
            num_heads=6,
            hidden_dim=384,
            mlp_dim=1536,
            dropout=0.0,
            attention_dropout=0.0,
            num_classes=1000,  # ä¸´æ—¶ï¼ŒåŽé¢ä¼šç§»é™¤
            representation_size=None,
        )
        
        # å¦‚æžœä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå°è¯•ä»Ž DINOv2 åŠ è½½ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if pretrained:
            try:
                # å°è¯•ä»Ž torch.hub åŠ è½½ DINOv2 ViT-S/16 é¢„è®­ç»ƒæƒé‡ï¼ˆå¦‚æžœæœ‰ï¼‰
                # æ³¨æ„ï¼šDINOv2 å®˜æ–¹åªæœ‰ S/14, B/14, L/14, g/14ï¼Œæ²¡æœ‰ S/16
                # è¿™é‡Œå°è¯•åŠ è½½ S/14 å¹¶é€‚é…åˆ° 16
                dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')
                _copy_dinov2_weights(backbone, dinov2_model)
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½ DINOv2 ViT-S/16 é¢„è®­ç»ƒæƒé‡: {e}")
                print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        
        feat_dim = backbone.heads.head.in_features
        backbone.heads = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´
        return backbone, feat_dim
    
    elif backbone_type == "vit_b_16":
        if pretrained:
            backbone = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)
        else:
            backbone = vit_b_16(weights=None)
        
        patch_size = 16
        default_image_size = 224
        
        # å¦‚æžœæŒ‡å®šäº† image_size ä¸”ä¸æ˜¯ 224ï¼Œéœ€è¦ä¿®æ”¹ ViT çš„ image_size
        if image_size is not None and image_size != default_image_size:
            backbone = _adapt_vit_image_size(backbone, image_size, patch_size, pretrained)
        
        # ViT çš„ CLS token ç»´åº¦
        feat_dim = backbone.heads.head.in_features
        backbone.heads = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´
        return backbone, feat_dim
    
    elif backbone_type == "vit_s_14":
        # ViT-S/14: Small model with patch_size=14
        # å‚æ•°é‡: ~22M
        
        # âœ… ä¿®å¤ï¼šå¯¹äºŽ DINOv2/DINO/iBOTï¼Œä½¿ç”¨å®˜æ–¹å®žçŽ°
        if use_dinov2_official:
            try:
                # ç›´æŽ¥ä½¿ç”¨ DINOv2 å®˜æ–¹çš„ ViT å®žçŽ°
                print("ðŸ”§ ä½¿ç”¨ DINOv2 å®˜æ–¹çš„ ViT-S/14 å®žçŽ°ï¼ˆè€Œéž torchvisionï¼‰")
                dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')
                
                # DINOv2 æ¨¡åž‹ç»“æž„ï¼šdinov2_model.student.backbone æˆ– dinov2_model.teacher.backbone
                # æˆ‘ä»¬ä½¿ç”¨ student çš„ backboneï¼ˆå› ä¸ºæˆ‘ä»¬è¦è®­ç»ƒå®ƒï¼‰
                if hasattr(dinov2_model, 'student') and hasattr(dinov2_model.student, 'backbone'):
                    backbone = dinov2_model.student.backbone
                elif hasattr(dinov2_model, 'backbone'):
                    backbone = dinov2_model.backbone
                else:
                    # å¦‚æžœç»“æž„ä¸åŒï¼Œå°è¯•ç›´æŽ¥ä½¿ç”¨æ¨¡åž‹
                    backbone = dinov2_model
                
                # èŽ·å–ç‰¹å¾ç»´åº¦
                # DINOv2 backbone é€šå¸¸æœ‰ embed_dim å±žæ€§ï¼ˆåœ¨ patch_embed æˆ– blocks ä¸­ï¼‰
                if hasattr(backbone, 'embed_dim'):
                    feat_dim = backbone.embed_dim
                elif hasattr(backbone, 'patch_embed') and hasattr(backbone.patch_embed, 'embed_dim'):
                    feat_dim = backbone.patch_embed.embed_dim
                elif hasattr(backbone, 'blocks') and len(backbone.blocks) > 0:
                    # ä»Žç¬¬ä¸€ä¸ª block èŽ·å–ç»´åº¦
                    if hasattr(backbone.blocks[0], 'embed_dim'):
                        feat_dim = backbone.blocks[0].embed_dim
                    elif hasattr(backbone.blocks[0], 'norm1') and hasattr(backbone.blocks[0].norm1, 'normalized_shape'):
                        feat_dim = backbone.blocks[0].norm1.normalized_shape[0]
                    else:
                        feat_dim = 384  # é»˜è®¤ ViT-S
                else:
                    feat_dim = 384  # é»˜è®¤ ViT-S
                
                # å¦‚æžœæŒ‡å®šäº† image_size ä¸”ä¸æ˜¯ 224ï¼Œéœ€è¦é€‚é…ä½ç½®ç¼–ç 
                if image_size is not None and image_size != 224:
                    backbone = _adapt_dinov2_image_size(backbone, image_size, patch_size=14)
                
                print(f"âœ… DINOv2 ViT-S/14 backbone æž„å»ºæˆåŠŸï¼Œç‰¹å¾ç»´åº¦: {feat_dim}")
                return backbone, feat_dim
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½ DINOv2 å®˜æ–¹ ViT-S/14: {e}")
                print("   å°†å›žé€€åˆ° torchvision VisionTransformer")
                import traceback
                traceback.print_exc()
                
                # âš ï¸ æ£€æŸ¥ï¼šå¦‚æžœå›žé€€åˆ° torchvisionï¼Œéœ€è¦ image_size èƒ½è¢« patch_size æ•´é™¤
                patch_size = 14
                if image_size is not None and image_size % patch_size != 0:
                    print(f"\nâŒ é”™è¯¯ï¼šDINOv2 å®˜æ–¹æ¨¡åž‹åŠ è½½å¤±è´¥ï¼Œå›žé€€åˆ° torchvision VisionTransformer")
                    print(f"   ä½† image_size={image_size} ä¸èƒ½è¢« patch_size={patch_size} æ•´é™¤ï¼")
                    print(f"\n   è§£å†³æ–¹æ¡ˆï¼š")
                    print(f"   1. âœ… æŽ¨èï¼šä½¿ç”¨ --img_size 112ï¼ˆ112 å¯ä»¥è¢« 14 æ•´é™¤ï¼Œæœ€æŽ¥è¿‘ 96ï¼‰")
                    print(f"   2. æˆ–è€…ä½¿ç”¨ --backbone_type vit_s_16ï¼ˆ96 å¯ä»¥è¢« 16 æ•´é™¤ï¼‰")
                    print(f"   3. æˆ–è€…æ£€æŸ¥ç½‘ç»œè¿žæŽ¥ï¼Œç¡®ä¿èƒ½åŠ è½½ DINOv2 å®˜æ–¹æ¨¡åž‹")
                    raise ValueError(
                        f"image_size={image_size} ä¸èƒ½è¢« patch_size={patch_size} æ•´é™¤ï¼"
                        f" è¯·ä½¿ç”¨ --img_size 112 æˆ– --backbone_type vit_s_16"
                    )
        
        # å›žé€€åˆ° torchvision VisionTransformerï¼ˆç”¨äºŽ SimCLR ç­‰å…¶ä»–æ–¹æ³•ï¼‰
        # âš ï¸ æ£€æŸ¥ï¼štorchvision VisionTransformer è¦æ±‚ image_size èƒ½è¢« patch_size æ•´é™¤
        patch_size = 14
        if image_size is not None and image_size % patch_size != 0:
            raise ValueError(
                f"âŒ é”™è¯¯ï¼šimage_size={image_size} ä¸èƒ½è¢« patch_size={patch_size} æ•´é™¤ï¼\n"
                f"   å¯¹äºŽ ViT-S/14ï¼Œimage_size å¿…é¡»æ˜¯ 14 çš„å€æ•°ï¼ˆå¦‚ 112, 224, 336 ç­‰ï¼‰\n"
                f"   å½“å‰ image_size={image_size}ï¼Œå»ºè®®æ”¹ä¸º 112ï¼ˆ96 æœ€æŽ¥è¿‘çš„ 14 çš„å€æ•°ï¼‰\n"
                f"   æˆ–è€…ä½¿ç”¨ --backbone_type vit_s_16ï¼ˆ96 å¯ä»¥è¢« 16 æ•´é™¤ï¼‰"
            )
        
        backbone = VisionTransformer(
            image_size=image_size or 224,
            patch_size=patch_size,
            num_layers=12,
            num_heads=6,
            hidden_dim=384,
            mlp_dim=1536,
            dropout=0.0,
            attention_dropout=0.0,
            num_classes=1000,  # ä¸´æ—¶ï¼ŒåŽé¢ä¼šç§»é™¤
            representation_size=None,
        )
        
        # å¦‚æžœä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå°è¯•ä»Ž DINOv2 åŠ è½½ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if pretrained:
            try:
                # å°è¯•ä»Ž torch.hub åŠ è½½ DINOv2 ViT-S/14 é¢„è®­ç»ƒæƒé‡
                dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')
                # å¤åˆ¶æƒé‡ï¼ˆéœ€è¦åŒ¹é…çš„å±‚ï¼‰
                _copy_dinov2_weights(backbone, dinov2_model)
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½ DINOv2 ViT-S/14 é¢„è®­ç»ƒæƒé‡: {e}")
                print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        
        feat_dim = backbone.heads.head.in_features
        backbone.heads = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´
        return backbone, feat_dim
    
    elif backbone_type == "vit_b_14":
        # ViT-B/14: Base model with patch_size=14
        # å‚æ•°é‡: ~86M
        
        # âœ… ä¿®å¤ï¼šå¯¹äºŽ DINOv2/DINO/iBOTï¼Œä½¿ç”¨å®˜æ–¹å®žçŽ°
        if use_dinov2_official:
            try:
                # ç›´æŽ¥ä½¿ç”¨ DINOv2 å®˜æ–¹çš„ ViT å®žçŽ°
                print("ðŸ”§ ä½¿ç”¨ DINOv2 å®˜æ–¹çš„ ViT-B/14 å®žçŽ°ï¼ˆè€Œéž torchvisionï¼‰")
                dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
                
                # DINOv2 æ¨¡åž‹ç»“æž„ï¼šdinov2_model.student.backbone æˆ– dinov2_model.teacher.backbone
                # æˆ‘ä»¬ä½¿ç”¨ student çš„ backboneï¼ˆå› ä¸ºæˆ‘ä»¬è¦è®­ç»ƒå®ƒï¼‰
                if hasattr(dinov2_model, 'student') and hasattr(dinov2_model.student, 'backbone'):
                    backbone = dinov2_model.student.backbone
                elif hasattr(dinov2_model, 'backbone'):
                    backbone = dinov2_model.backbone
                else:
                    # å¦‚æžœç»“æž„ä¸åŒï¼Œå°è¯•ç›´æŽ¥ä½¿ç”¨æ¨¡åž‹
                    backbone = dinov2_model
                
                # èŽ·å–ç‰¹å¾ç»´åº¦
                # DINOv2 backbone é€šå¸¸æœ‰ embed_dim å±žæ€§ï¼ˆåœ¨ patch_embed æˆ– blocks ä¸­ï¼‰
                if hasattr(backbone, 'embed_dim'):
                    feat_dim = backbone.embed_dim
                elif hasattr(backbone, 'patch_embed') and hasattr(backbone.patch_embed, 'embed_dim'):
                    feat_dim = backbone.patch_embed.embed_dim
                elif hasattr(backbone, 'blocks') and len(backbone.blocks) > 0:
                    # ä»Žç¬¬ä¸€ä¸ª block èŽ·å–ç»´åº¦
                    if hasattr(backbone.blocks[0], 'embed_dim'):
                        feat_dim = backbone.blocks[0].embed_dim
                    elif hasattr(backbone.blocks[0], 'norm1') and hasattr(backbone.blocks[0].norm1, 'normalized_shape'):
                        feat_dim = backbone.blocks[0].norm1.normalized_shape[0]
                    else:
                        feat_dim = 768  # é»˜è®¤ ViT-B
                else:
                    feat_dim = 768  # é»˜è®¤ ViT-B
                
                # å¦‚æžœæŒ‡å®šäº† image_size ä¸”ä¸æ˜¯ 224ï¼Œéœ€è¦é€‚é…ä½ç½®ç¼–ç 
                if image_size is not None and image_size != 224:
                    backbone = _adapt_dinov2_image_size(backbone, image_size, patch_size=14)
                
                print(f"âœ… DINOv2 ViT-B/14 backbone æž„å»ºæˆåŠŸï¼Œç‰¹å¾ç»´åº¦: {feat_dim}")
                return backbone, feat_dim
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½ DINOv2 å®˜æ–¹ ViT-B/14: {e}")
                print("   å°†å›žé€€åˆ° torchvision VisionTransformer")
                import traceback
                traceback.print_exc()
        
        # å›žé€€åˆ° torchvision VisionTransformerï¼ˆç”¨äºŽ SimCLR ç­‰å…¶ä»–æ–¹æ³•ï¼‰
        backbone = VisionTransformer(
            image_size=image_size or 224,
            patch_size=14,
            num_layers=12,
            num_heads=12,
            hidden_dim=768,
            mlp_dim=3072,
            dropout=0.0,
            attention_dropout=0.0,
            num_classes=1000,  # ä¸´æ—¶ï¼ŒåŽé¢ä¼šç§»é™¤
            representation_size=None,
        )
        
        # å¦‚æžœä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå°è¯•ä»Ž DINOv2 åŠ è½½ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if pretrained:
            try:
                # å°è¯•ä»Ž torch.hub åŠ è½½ DINOv2 ViT-B/14 é¢„è®­ç»ƒæƒé‡
                dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')
                # å¤åˆ¶æƒé‡ï¼ˆéœ€è¦åŒ¹é…çš„å±‚ï¼‰
                _copy_dinov2_weights(backbone, dinov2_model)
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½ DINOv2 ViT-B/14 é¢„è®­ç»ƒæƒé‡: {e}")
                print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æƒé‡")
        
        feat_dim = backbone.heads.head.in_features
        backbone.heads = nn.Identity()  # ç§»é™¤åˆ†ç±»å¤´
        return backbone, feat_dim
    
    else:
        raise ValueError(f"Unknown backbone_type: {backbone_type}. Available: resnet50, vit_s_16, vit_b_16, vit_s_14, vit_b_14")


def _adapt_vit_image_size(backbone, image_size, patch_size, pretrained):
    """é€‚é… ViT åˆ°ä¸åŒçš„å›¾åƒå°ºå¯¸"""
    backbone.image_size = image_size
    num_patches = (image_size // patch_size) ** 2
    
    # æ›´æ–°ä½ç½®ç¼–ç çš„å°ºå¯¸
    if hasattr(backbone, 'encoder') and hasattr(backbone.encoder, 'pos_embedding'):
        embed_dim = backbone.encoder.pos_embedding.shape[-1]
        new_pos_embedding = nn.Parameter(
            torch.zeros(1, num_patches + 1, embed_dim)  # +1 for CLS token
        )
        # å¦‚æžœä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå°è¯•æ’å€¼æ—§çš„ä½ç½®ç¼–ç 
        if pretrained and backbone.encoder.pos_embedding.shape[1] > 1:
            old_num_patches = backbone.encoder.pos_embedding.shape[1] - 1
            old_pos = backbone.encoder.pos_embedding[:, 1:].reshape(
                1, int(old_num_patches ** 0.5), int(old_num_patches ** 0.5), embed_dim
            ).permute(0, 3, 1, 2)
            new_pos = nn.functional.interpolate(
                old_pos, 
                size=(int(num_patches ** 0.5), int(num_patches ** 0.5)), 
                mode='bilinear', 
                align_corners=False
            ).permute(0, 2, 3, 1).reshape(1, num_patches, embed_dim)
            new_pos_embedding.data[:, 1:] = new_pos
            new_pos_embedding.data[:, 0] = backbone.encoder.pos_embedding.data[:, 0]  # CLS token
        backbone.encoder.pos_embedding = new_pos_embedding
    
    return backbone


def _adapt_dinov2_image_size(backbone, image_size, patch_size=14):
    """é€‚é… DINOv2 backbone åˆ°ä¸åŒçš„å›¾åƒå°ºå¯¸"""
    # DINOv2 backbone ç›´æŽ¥æœ‰ pos_embed å±žæ€§
    if hasattr(backbone, 'pos_embed'):
        embed_dim = backbone.pos_embed.shape[-1]
        num_patches = (image_size // patch_size) ** 2
        
        # åˆ›å»ºæ–°çš„ä½ç½®ç¼–ç 
        new_pos_embed = nn.Parameter(
            torch.zeros(1, num_patches + 1, embed_dim)  # +1 for CLS token
        )
        
        # å¦‚æžœæ—§çš„ä½ç½®ç¼–ç å­˜åœ¨ï¼Œå°è¯•æ’å€¼
        if backbone.pos_embed.shape[1] > 1:
            old_num_patches = backbone.pos_embed.shape[1] - 1
            old_size = int(old_num_patches ** 0.5)
            new_size = int(num_patches ** 0.5)
            
            if old_size > 0:
                old_pos = backbone.pos_embed[:, 1:].reshape(
                    1, old_size, old_size, embed_dim
                ).permute(0, 3, 1, 2)
                new_pos = nn.functional.interpolate(
                    old_pos,
                    size=(new_size, new_size),
                    mode='bilinear',
                    align_corners=False
                ).permute(0, 2, 3, 1).reshape(1, num_patches, embed_dim)
                new_pos_embed.data[:, 1:] = new_pos
                new_pos_embed.data[:, 0] = backbone.pos_embed.data[:, 0]  # CLS token
        
        backbone.pos_embed = new_pos_embed
    
    return backbone


def _copy_dinov2_weights(target_model, source_model):
    """ä»Ž DINOv2 æ¨¡åž‹å¤åˆ¶æƒé‡åˆ°ç›®æ ‡æ¨¡åž‹"""
    # DINOv2 æ¨¡åž‹çš„ç»“æž„å¯èƒ½ä¸Žæ ‡å‡† ViT ç•¥æœ‰ä¸åŒ
    # è¿™é‡Œå°è¯•åŒ¹é…å¯ç”¨çš„å±‚
    target_state = target_model.state_dict()
    source_state = source_model.state_dict()
    
    matched_keys = []
    for key in target_state.keys():
        if key in source_state:
            if target_state[key].shape == source_state[key].shape:
                target_state[key] = source_state[key]
                matched_keys.append(key)
    
    target_model.load_state_dict(target_state)
    print(f"âœ” æˆåŠŸåŠ è½½ {len(matched_keys)}/{len(target_state)} å±‚æƒé‡")


# ============================================================
# 2. Optimizer & Learning Rate Scheduler
# ============================================================

def build_optimizer(model, optimizer_type="adamw", lr=1e-3, weight_decay=1e-4, **kwargs):
    """
    æž„å»ºä¼˜åŒ–å™¨
    
    Args:
        model: æ¨¡åž‹
        optimizer_type: "adamw" æˆ– "sgd"
        lr: å­¦ä¹ çŽ‡
        weight_decay: æƒé‡è¡°å‡
        **kwargs: å…¶ä»–ä¼˜åŒ–å™¨å‚æ•°
    
    Returns:
        optimizer: torch.optim.Optimizer
    """
    if optimizer_type == "adamw":
        return torch.optim.AdamW(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            **kwargs
        )
    elif optimizer_type == "sgd":
        momentum = kwargs.get("momentum", 0.9)
        return torch.optim.SGD(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            momentum=momentum,
            **kwargs
        )
    else:
        raise ValueError(f"Unknown optimizer_type: {optimizer_type}")


def build_scheduler(optimizer, scheduler_type="cosine", T_max=100, warmup_epochs=0, **kwargs):
    """
    æž„å»ºå­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼ˆæ”¯æŒ warmupï¼‰
    
    Args:
        optimizer: ä¼˜åŒ–å™¨
        scheduler_type: "cosine" æˆ– "step"
        T_max: æ€»è®­ç»ƒè½®æ•°ï¼ˆcosineï¼‰æˆ– step_sizeï¼ˆstepï¼‰
        warmup_epochs: warmup è½®æ•°
        **kwargs: å…¶ä»–è°ƒåº¦å™¨å‚æ•°
    
    Returns:
        scheduler: å­¦ä¹ çŽ‡è°ƒåº¦å™¨
    """
    if scheduler_type == "cosine":
        base_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=T_max, **kwargs
        )
    elif scheduler_type == "step":
        step_size = kwargs.get("step_size", 30)
        gamma = kwargs.get("gamma", 0.1)
        base_scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, step_size=step_size, gamma=gamma
        )
    else:
        raise ValueError(f"Unknown scheduler_type: {scheduler_type}")
    
    # å¦‚æžœæœ‰ warmupï¼ŒåŒ…è£…æˆ WarmupScheduler
    if warmup_epochs > 0:
        return WarmupScheduler(base_scheduler, warmup_epochs=warmup_epochs)
    
    return base_scheduler


class WarmupScheduler:
    """Warmup å­¦ä¹ çŽ‡è°ƒåº¦å™¨åŒ…è£…å™¨"""
    
    def __init__(self, scheduler, warmup_epochs=10):
        self.scheduler = scheduler
        self.warmup_epochs = warmup_epochs
        self.base_lrs = [group['lr'] for group in scheduler.optimizer.param_groups]
        self.current_epoch = 0
    
    def step(self):
        if self.current_epoch < self.warmup_epochs:
            # Warmup: çº¿æ€§å¢žé•¿
            for param_group, base_lr in zip(self.scheduler.optimizer.param_groups, self.base_lrs):
                param_group['lr'] = base_lr * (self.current_epoch + 1) / self.warmup_epochs
        else:
            self.scheduler.step()
        
        self.current_epoch += 1
    
    def get_last_lr(self):
        return [group['lr'] for group in self.scheduler.optimizer.param_groups]


# ============================================================
# 3. å…¶ä»–é€šç”¨å·¥å…·
# ============================================================

def count_parameters(model):
    """ç»Ÿè®¡æ¨¡åž‹å‚æ•°é‡"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def save_checkpoint(save_path, epoch, model, optimizer, scheduler=None, **kwargs):
    """ä¿å­˜ checkpoint"""
    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        **kwargs
    }
    
    if scheduler is not None:
        checkpoint["scheduler_state_dict"] = scheduler.scheduler.state_dict() if hasattr(scheduler, 'scheduler') else scheduler.state_dict()
    
    torch.save(checkpoint, save_path)
    return save_path


def load_checkpoint(load_path, model, optimizer=None, scheduler=None, device=None):
    """åŠ è½½ checkpoint"""
    checkpoint = torch.load(load_path, map_location=device)
    
    model.load_state_dict(checkpoint["model_state_dict"])
    
    if optimizer is not None and "optimizer_state_dict" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
    
    if scheduler is not None and "scheduler_state_dict" in checkpoint:
        if hasattr(scheduler, 'scheduler'):
            scheduler.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
        else:
            scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
    
    return checkpoint.get("epoch", 0), checkpoint

