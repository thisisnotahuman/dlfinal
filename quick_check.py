"""
å¿«é€Ÿæ£€æŸ¥ï¼šéªŒè¯è®­ç»ƒæ˜¯å¦çœŸçš„åœ¨æ›´æ–° backbone
"""

import torch
import torch.nn as nn

def check_backbone_gradients(method, device):
    """æ£€æŸ¥ backbone æ˜¯å¦æœ‰æ¢¯åº¦"""
    method.train()
    
    # åˆ›å»ºä¸€ä¸ª dummy batch
    dummy_batch = torch.randn(2, 3, 96, 96).to(device)
    views = torch.stack([dummy_batch, dummy_batch], dim=1)  # [2, 2, 3, 96, 96]
    
    # å‰å‘ä¼ æ’­
    loss, _ = method.compute_loss(views)
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ£€æŸ¥ backbone å‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦
    backbone_has_grad = False
    backbone_param_count = 0
    for name, param in method.backbone.named_parameters():
        backbone_param_count += 1
        if param.grad is not None:
            backbone_has_grad = True
            grad_norm = param.grad.norm().item()
            print(f"âœ… Backbone å‚æ•° '{name}' æœ‰æ¢¯åº¦ï¼Œæ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
            break
    
    if not backbone_has_grad:
        print(f"âŒ è­¦å‘Šï¼šæ£€æŸ¥äº† {backbone_param_count} ä¸ª backbone å‚æ•°ï¼Œéƒ½æ²¡æœ‰æ¢¯åº¦ï¼")
        print("   è¿™å¯èƒ½æ„å‘³ç€ backbone è¢«å†»ç»“äº†ï¼Œæˆ–è€…æ¢¯åº¦æ²¡æœ‰æ­£ç¡®ä¼ æ’­")
    else:
        print(f"âœ… Backbone æœ‰æ¢¯åº¦ï¼Œè®­ç»ƒåº”è¯¥æ­£å¸¸")
    
    # æ£€æŸ¥ head å‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦
    head_has_grad = False
    head_param_count = 0
    for name, param in method.head.named_parameters():
        head_param_count += 1
        if param.grad is not None:
            head_has_grad = True
            grad_norm = param.grad.norm().item()
            print(f"âœ… Head å‚æ•° '{name}' æœ‰æ¢¯åº¦ï¼Œæ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
            break
    
    if not head_has_grad:
        print(f"âŒ è­¦å‘Šï¼šæ£€æŸ¥äº† {head_param_count} ä¸ª head å‚æ•°ï¼Œéƒ½æ²¡æœ‰æ¢¯åº¦ï¼")
    else:
        print(f"âœ… Head æœ‰æ¢¯åº¦ï¼Œè®­ç»ƒåº”è¯¥æ­£å¸¸")
    
    return backbone_has_grad and head_has_grad


def check_optimizer_params(optimizer, method):
    """æ£€æŸ¥ä¼˜åŒ–å™¨ä¸­æ˜¯å¦åŒ…å« backbone å‚æ•°"""
    optimizer_param_ids = set(id(p) for group in optimizer.param_groups for p in group['params'])
    
    backbone_param_ids = set(id(p) for p in method.backbone.parameters())
    head_param_ids = set(id(p) for p in method.head.parameters())
    
    backbone_in_optimizer = len(backbone_param_ids & optimizer_param_ids) > 0
    head_in_optimizer = len(head_param_ids & optimizer_param_ids) > 0
    
    print(f"\nğŸ“Š ä¼˜åŒ–å™¨å‚æ•°æ£€æŸ¥ï¼š")
    print(f"   Backbone å‚æ•°åœ¨ä¼˜åŒ–å™¨ä¸­: {'âœ… æ˜¯' if backbone_in_optimizer else 'âŒ å¦'}")
    print(f"   Head å‚æ•°åœ¨ä¼˜åŒ–å™¨ä¸­: {'âœ… æ˜¯' if head_in_optimizer else 'âŒ å¦'}")
    
    if not backbone_in_optimizer:
        print("   âš ï¸  è­¦å‘Šï¼šBackbone å‚æ•°ä¸åœ¨ä¼˜åŒ–å™¨ä¸­ï¼Œä¸ä¼šè¢«æ›´æ–°ï¼")
    
    return backbone_in_optimizer and head_in_optimizer


if __name__ == "__main__":
    print("="*60)
    print("ğŸ” å¿«é€Ÿæ£€æŸ¥ï¼šè®­ç»ƒä»£ç æ˜¯å¦æ­£ç¡®")
    print("="*60)
    
    # è¿™é‡Œéœ€è¦å¯¼å…¥ä½ çš„å®é™…ä»£ç 
    # ç”±äºåœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œï¼Œä½ éœ€è¦ä¿®æ”¹è¿™ä¸ªè„šæœ¬æ¥é€‚é…ä½ çš„ç¯å¢ƒ
    print("\nâš ï¸  è¿™ä¸ªè„šæœ¬éœ€è¦åœ¨ä½ çš„è®­ç»ƒç¯å¢ƒä¸­è¿è¡Œ")
    print("   è¯·å°†ä»¥ä¸‹ä»£ç æ·»åŠ åˆ°ä½ çš„è®­ç»ƒè„šæœ¬ä¸­ï¼Œåœ¨è®­ç»ƒå¼€å§‹å‰æ£€æŸ¥ï¼š")
    print()
    print("""
    # åœ¨æ„å»º optimizer ä¹‹åï¼Œè®­ç»ƒå¼€å§‹å‰æ·»åŠ ï¼š
    from quick_check import check_backbone_gradients, check_optimizer_params
    
    # æ£€æŸ¥ä¼˜åŒ–å™¨
    check_optimizer_params(optimizer, method)
    
    # æ£€æŸ¥æ¢¯åº¦
    check_backbone_gradients(method, device)
    """)

