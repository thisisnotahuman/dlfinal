"""
é«˜æ€§èƒ½ SimCLR / DINO æ•°æ®åŠ è½½ + GPU å¼ºå¢å¼º
==================================================
ç‰¹ç‚¹ï¼š
- å…¨éƒ¨å¢å¼ºåœ¨ GPU ä¸Šæ‰¹å¤„ç†ï¼ˆtorchvision v2ï¼‰
- æ—  PILã€æ—  for-loop
- æ”¯æŒ --train_sample ç”¨äºå¿«é€Ÿå­é›†è®­ç»ƒ
- DataLoader å¤šçº¿ç¨‹ + prefetch ä¼˜åŒ–
"""

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, Subset
import torchvision.transforms.v2 as v2
from torchvision.models import ResNeXt50_32X4D_Weights
from datasets import load_dataset
import random


# ============================================================
# 1. GPU SimCLR åŒè§†å›¾å¢å¼º
# ============================================================

def build_two_view_augment(
    img_size: int,
    strength="strong",
    mean=(0.485, 0.456, 0.406),
    std=(0.229, 0.224, 0.225),
    method="simclr",  # æ–°å¢ï¼šæ–¹æ³•ç±»å‹
    num_local_crops=0,  # æ–°å¢ï¼šlocal crops æ•°é‡ï¼ˆDINOv2 ä½¿ç”¨ï¼‰
):
    """
    æ„å»ºåŒè§†å›¾æˆ–å¤šè§†å›¾å¢å¼º
    
    è¾“å…¥ï¼š x = [B, 3, H, W]ï¼Œfloat32 GPUï¼Œå·²ç»æ˜¯ [0, 1] èŒƒå›´ï¼ˆç»è¿‡ ToTensorï¼‰
    è¾“å‡ºï¼š 
        - SimCLR: [B, 2, 3, H, W]
        - DINOv2: [B, 2+num_local_crops, 3, H, W]ï¼ˆå‰2ä¸ªæ˜¯globalï¼Œåé¢æ˜¯localï¼‰
    
    æµç¨‹ï¼š
    1. è¾“å…¥å·²ç»æ˜¯ [0, 1] èŒƒå›´ï¼ˆä» ms_transform çš„ ToTensor å¾—åˆ°ï¼‰
    2. åœ¨ [0, 1] èŒƒå›´ä¸Šè¿›è¡Œå¢å¼ºæ“ä½œ
    3. æœ€å normalize åˆ°ç›®æ ‡åˆ†å¸ƒ
    
    DINOv2 å®˜æ–¹ multi-crop è§„èŒƒï¼š
    - Global crops (2ä¸ª): å°ºå¯¸ 224, crop scale (0.4, 1.0), ç”¨äº teacher + student
    - Local crops (6-10ä¸ª): å°ºå¯¸ 96, crop scale (0.05, 0.4), ä»…ç”¨äº student
    """
    
    if strength.lower() not in {"strong", "medium", "weak"}:
        raise ValueError(f"strength must be strong/medium/weak, got {strength!r}")
    
    strength = strength.lower()
    
    # è·å– ImageNet çš„ mean/stdï¼ˆç”¨äºå‚è€ƒï¼Œä½†æœ€ç»ˆä½¿ç”¨ä¼ å…¥çš„ mean/stdï¼‰
    weights = ResNeXt50_32X4D_Weights.IMAGENET1K_V1
    base_mean = torch.tensor(weights.transforms().mean).view(1, 3, 1, 1)
    base_std = torch.tensor(weights.transforms().std).view(1, 3, 1, 1)
    
    # ç›®æ ‡ normalize çš„ mean/std
    mean_t = torch.tensor(mean).view(1, 3, 1, 1)
    std_t = torch.tensor(std).view(1, 3, 1, 1)
    
    # å¢å¼ºé…ç½®
    # DINOv2 å®˜æ–¹è¦æ±‚ï¼š
    # - Global view 1: blur p=1.0, solarize p=0
    # - Global view 2: blur p=0.0, solarize p=0.2
    # - Local crops: blur p=0.5, solarize p=0
    is_dinov2 = method.lower() in ["dino", "dinov2", "ibot"]
    
    if strength == "strong":
        if is_dinov2:
            # DINOv2 å®˜æ–¹é…ç½®
            aug_cfg = dict(
                use_rrc=True,
                cj=(0.8, 0.8, 0.8, 0.2),
                p_cj=0.8,
                p_gray=0.2,
                blur_p1=1.0,  # Global view 1: blur p=1.0
                blur_p2=0.0,  # âœ… ä¿®å¤ï¼šGlobal view 2: blur p=0.0ï¼ˆä¸æ˜¯ 0.1ï¼‰
                solarize_p2=0.2,  # Global view 2: solarize p=0.2
                blur_p_local=0.5,  # âœ… æ–°å¢ï¼šLocal crops: blur p=0.5
                solarize_p_local=0.0,  # âœ… æ–°å¢ï¼šLocal crops: solarize p=0
            )
        else:
            # SimCLR æˆ–å…¶ä»–æ–¹æ³•çš„é…ç½®
            aug_cfg = dict(
                use_rrc=True,
                cj=(0.8, 0.8, 0.8, 0.2),
                p_cj=0.8,
                p_gray=0.2,
                blur_p1=1.0,
                blur_p2=0.1,
                solarize_p2=0.2,
            )
    elif strength == "medium":
        if is_dinov2:
            # DINOv2 å®˜æ–¹é…ç½®ï¼ˆmedium å¼ºåº¦æ—¶ä¹Ÿéµå¾ªç›¸åŒæ¨¡å¼ï¼‰
            aug_cfg = dict(
                use_rrc=True,
                cj=(0.4, 0.4, 0.3, 0.1),
                p_cj=0.7,
                p_gray=0.15,
                blur_p1=1.0,  # Global view 1: blur p=1.0
                blur_p2=0.0,  # âœ… ä¿®å¤ï¼šGlobal view 2: blur p=0.0
                solarize_p2=0.2,  # Global view 2: solarize p=0.2
                blur_p_local=0.5,  # Local crops: blur p=0.5
                solarize_p_local=0.0,  # Local crops: solarize p=0
            )
        else:
            aug_cfg = dict(
                use_rrc=True,
                cj=(0.4, 0.4, 0.3, 0.1),
                p_cj=0.7,
                p_gray=0.15,
                blur_p1=0.6,
                blur_p2=0.1,
                solarize_p2=0.0,
            )
    else:  # weak
        if is_dinov2:
            # DINOv2 å®˜æ–¹é…ç½®ï¼ˆweak å¼ºåº¦æ—¶ä¹Ÿéµå¾ªç›¸åŒæ¨¡å¼ï¼‰
            aug_cfg = dict(
                use_rrc=True,
                cj=(0.3, 0.3, 0.2, 0.05),
                p_cj=0.5,
                p_gray=0.1,
                blur_p1=1.0,  # Global view 1: blur p=1.0
                blur_p2=0.0,  # âœ… ä¿®å¤ï¼šGlobal view 2: blur p=0.0
                solarize_p2=0.2,  # Global view 2: solarize p=0.2
                blur_p_local=0.5,  # Local crops: blur p=0.5
                solarize_p_local=0.0,  # Local crops: solarize p=0
            )
        else:
            aug_cfg = dict(
                use_rrc=True,
                cj=(0.3, 0.3, 0.2, 0.05),
                p_cj=0.5,
                p_gray=0.1,
                blur_p1=0.3,
                blur_p2=0.0,
                solarize_p2=0.0,
            )
    if is_dinov2:
        # DINOv2 å®˜æ–¹è§„èŒƒï¼ˆImageNet 224x224ï¼‰ï¼š
        # - Global crops (2ä¸ª): å°ºå¯¸ 224, crop scale (0.4, 1.0), ç”¨äº teacher + student
        # - Local crops (6-10ä¸ª): å°ºå¯¸ 96, crop scale (0.05, 0.4), ä»…ç”¨äº student
        # - æ¯”ä¾‹å…³ç³»ï¼šGlobal 224 / Local 96 â‰ˆ 2.33
        
        # âœ… ä¿®å¤ï¼šæ ¹æ®åŸå§‹å›¾åƒå°ºå¯¸è‡ªé€‚åº”è°ƒæ•´
        # å¦‚æœåŸå§‹æ•°æ®æ˜¯ 96x96ï¼ŒæŒ‰æ¯”ä¾‹ï¼š
        # - Global crops: 96x96ï¼ˆä» 96x96 ä¸­ cropï¼Œscale (0.4, 1.0)ï¼‰
        # - Local crops: åº”è¯¥æ›´å°ï¼Œæ¯”å¦‚ 32x32 æˆ– 48x48ï¼ˆä¿æŒæ¯”ä¾‹å…³ç³»ï¼‰
        global_crop_scale = (0.4, 1.0)  # âœ… DINOv2 å®˜æ–¹ global crop scale
        
        # âœ… ä¿®å¤ï¼šæŒ‰å®ç°è¯´æ˜ï¼Œæ‰€æœ‰è§†å›¾ï¼ˆåŒ…æ‹¬localï¼‰åº”è¯¥ç»Ÿä¸€å°ºå¯¸ä¸º img_size
        # è¯´æ˜è¦æ±‚ï¼šæ‰€æœ‰è§†å›¾éƒ½æ˜¯ 112Ã—112ï¼ˆé€‚é…åï¼‰ï¼ŒåŒ…æ‹¬ local
        # ä½† local çš„ crop scale æ›´å° (0.1~0.5)ï¼Œç„¶å resize åˆ°ç»Ÿä¸€å°ºå¯¸
        # è¿™æ ·æ—¢ä¿æŒäº†å¤šå°ºåº¦ä¿¡æ¯ï¼Œåˆç»Ÿä¸€äº†è¾“å…¥å°ºå¯¸
        if img_size >= 224:
            # æ ‡å‡† DINOv2 é…ç½®ï¼šlocal crop 96ï¼Œä½† resize åˆ° 224
            local_crop_size = 96
        else:
            # å¯¹äºè¾ƒå°çš„ img_sizeï¼ˆå¦‚ 96ï¼‰ï¼Œlocal crop ä¹Ÿ resize åˆ° img_size
            # ä½¿ç”¨æ›´å°çš„ crop size æ¥ cropï¼Œç„¶å resize åˆ° img_size
            # è¿™æ ·å¯ä»¥ä¿æŒå¤šå°ºåº¦ä¿¡æ¯ï¼ŒåŒæ—¶ç»Ÿä¸€è¾“å…¥å°ºå¯¸
            local_crop_size = max(32, img_size // 3)  # è‡³å°‘ 32Ã—32ï¼Œç„¶å resize åˆ° img_size
        
        local_crop_scale = (0.05, 0.4)  # âœ… DINOv2 å®˜æ–¹ local crop scale
        if num_local_crops == 0:
            num_local_crops = 6  # âœ… ä¿®å¤ï¼šæŒ‰å®ç°è¯´æ˜ï¼Œä½¿ç”¨ 6 ä¸ª local cropsï¼ˆä¸æ˜¯ 8 ä¸ªï¼‰
        
        print(f"ğŸ“ DINOv2 crop é…ç½®: Global={img_size}Ã—{img_size}, Local crop={local_crop_size}Ã—{local_crop_size}â†’resizeåˆ°{img_size}Ã—{img_size}ï¼ˆç»Ÿä¸€å°ºå¯¸ï¼‰")
    else:
        # SimCLR æˆ–å…¶ä»–ï¼šä½¿ç”¨åŸæ¥çš„ scale
        global_crop_scale = (0.2, 1.0)
        local_crop_size = img_size // 3  # ä¸ä½¿ç”¨ï¼Œä½†å®šä¹‰ä»¥é¿å…é”™è¯¯
        local_crop_scale = (0.05, 0.2)
        num_local_crops = 0  # SimCLR ä¸ä½¿ç”¨ local crops
    
    # æ„å»ºå¢å¼º pipelineï¼ˆåœ¨ [0, 1] èŒƒå›´ä¸Šæ“ä½œï¼‰
    def build_global_pipeline(view_idx: int):
        """æ„å»º global crop çš„å¢å¼º pipeline"""
        ops = []
        
        if aug_cfg["use_rrc"]:
            # ä½¿ç”¨æ­£ç¡®çš„ crop scale
            ops.append(v2.RandomResizedCrop(img_size, scale=global_crop_scale))
        else:
            ops.extend([
                v2.Resize(img_size),
                v2.CenterCrop(img_size),
            ])
        
        ops.append(v2.RandomHorizontalFlip(0.5))
        
        # ColorJitterï¼ˆå¸¦æ¦‚ç‡ï¼‰
        if aug_cfg["p_cj"] > 0:
            ops.append(v2.RandomApply(
                [v2.ColorJitter(*aug_cfg["cj"])],
                p=aug_cfg["p_cj"]
            ))
        
        # Grayscale
        if aug_cfg["p_gray"] > 0:
            ops.append(v2.RandomGrayscale(aug_cfg["p_gray"]))
        
        # GaussianBlur
        blur_p = aug_cfg["blur_p1"] if view_idx == 0 else aug_cfg["blur_p2"]
        if blur_p > 0:
            k = max(3, int(0.1 * img_size) // 2 * 2 + 1)
            ops.append(v2.RandomApply(
                [v2.GaussianBlur(kernel_size=k, sigma=(0.1, 2.0))],
                p=blur_p
            ))
        
        # Solarizeï¼ˆä»… view2ï¼‰
        if view_idx == 1 and aug_cfg["solarize_p2"] > 0:
            ops.append(v2.RandomApply(
                [v2.RandomSolarize(0.2)],
                p=aug_cfg["solarize_p2"]
            ))
        
        return v2.Compose(ops)
    
    def build_local_pipeline():
        """æ„å»º local crop çš„å¢å¼º pipelineï¼ˆæ›´å¼±çš„å¢å¼ºï¼‰"""
        ops = []
        
        # Local crop: ä½¿ç”¨æ›´å°çš„ scale
        ops.append(v2.RandomResizedCrop(local_crop_size, scale=local_crop_scale))
        ops.append(v2.RandomHorizontalFlip(0.5))
        
        # Local crops ä½¿ç”¨æ›´å¼±çš„é¢œè‰²æŠ–åŠ¨
        if aug_cfg["p_cj"] > 0:
            cj_weak = tuple(x * 0.5 for x in aug_cfg["cj"])
            ops.append(v2.RandomApply(
                [v2.ColorJitter(*cj_weak)],
                p=aug_cfg["p_cj"] * 0.5
            ))
        
        # âœ… DINOv2 å®˜æ–¹è¦æ±‚ï¼šLocal crops ä½¿ç”¨ blur p=0.5
        if is_dinov2 and "blur_p_local" in aug_cfg:
            blur_p_local = aug_cfg.get("blur_p_local", 0.5)
            if blur_p_local > 0:
                k = max(3, int(0.1 * local_crop_size) // 2 * 2 + 1)
                ops.append(v2.RandomApply(
                    [v2.GaussianBlur(kernel_size=k, sigma=(0.1, 2.0))],
                    p=blur_p_local
                ))
        
        # âœ… DINOv2 å®˜æ–¹è¦æ±‚ï¼šLocal crops ä¸ä½¿ç”¨ solarize (p=0)
        # å·²ç»åœ¨ aug_cfg ä¸­è®¾ç½®ä¸º 0ï¼Œä¸éœ€è¦æ·»åŠ 
        
        return v2.Compose(ops)
    
    pip_v1 = build_global_pipeline(0)
    pip_v2 = build_global_pipeline(1)
    
    if is_dinov2 and num_local_crops > 0:
        local_pip = build_local_pipeline()
    
    def apply(x):
        """
        Args:
            x: [B, 3, H, W]ï¼Œ[0, 1] èŒƒå›´çš„ tensorï¼ˆGPUï¼‰
        
        Returns:
            SimCLR: [B, 2, 3, H, W]ï¼ˆä¸¤ä¸ªéƒ½æ˜¯ img_sizeÃ—img_sizeï¼‰
            DINOv2: [B, 2+num_local_crops, 3, H, W]ï¼ˆæ‰€æœ‰è§†å›¾éƒ½æ˜¯ img_sizeÃ—img_sizeï¼Œç»Ÿä¸€å°ºå¯¸ï¼‰
        """
        device = x.device
        mean_base = base_mean.to(device)
        std_base = base_std.to(device)
        mean_custom = mean_t.to(device)
        std_custom = std_t.to(device)
        
        # è¾“å…¥ x å·²ç»æ˜¯ [0, 1] èŒƒå›´ï¼ˆä» ToTensor å¾—åˆ°ï¼‰
        x = torch.clamp(x, 0.0, 1.0)
        
        views = []
        
        # ç”Ÿæˆ 2 ä¸ª global crops
        v1 = pip_v1(x)  # [B, 3, img_size, img_size]
        v2 = pip_v2(x)  # [B, 3, img_size, img_size]
        v1 = torch.clamp(v1, 0.0, 1.0)
        v2 = torch.clamp(v2, 0.0, 1.0)
        v1 = (v1 - mean_custom) / std_custom
        v2 = (v2 - mean_custom) / std_custom
        views.append(v1)
        views.append(v2)
        
        # DINOv2: ç”Ÿæˆ local crops
        if is_dinov2 and num_local_crops > 0:
            for _ in range(num_local_crops):
                v_local = local_pip(x)  # [B, 3, local_crop_size, local_crop_size]
                v_local = torch.clamp(v_local, 0.0, 1.0)
                
                # âœ… ä¿®å¤ï¼šæŒ‰å®ç°è¯´æ˜ï¼Œæ‰€æœ‰è§†å›¾ï¼ˆåŒ…æ‹¬localï¼‰åº”è¯¥ç»Ÿä¸€å°ºå¯¸ä¸º img_size
                # Local crop ä»æ›´å°çš„åŒºåŸŸ cropï¼ˆscale 0.1~0.5ï¼‰ï¼Œç„¶å resize åˆ° img_size
                # è¿™æ ·æ—¢ä¿æŒäº†å¤šå°ºåº¦ä¿¡æ¯ï¼Œåˆç»Ÿä¸€äº†è¾“å…¥å°ºå¯¸ï¼Œä¾¿äº stack
                try:
                    v_local = F.interpolate(
                        v_local,  # [B, 3, local_crop_size, local_crop_size]
                        size=(img_size, img_size),
                        mode='bilinear',
                        align_corners=False,
                        antialias=True
                    )  # [B, 3, img_size, img_size]
                except TypeError:
                    # å¦‚æœ antialias ä¸æ”¯æŒï¼Œä½¿ç”¨ä¸å¸¦ antialias çš„ç‰ˆæœ¬
                    v_local = F.interpolate(
                        v_local,
                        size=(img_size, img_size),
                        mode='bilinear',
                        align_corners=False
                    )
                v_local = (v_local - mean_custom) / std_custom
                views.append(v_local)  # [B, 3, img_size, img_size]ï¼ˆç»Ÿä¸€å°ºå¯¸ï¼‰
        
        # å †å æ‰€æœ‰ viewsï¼ˆç°åœ¨æ‰€æœ‰ views éƒ½æ˜¯ç»Ÿä¸€å°ºå¯¸ï¼Œå¯ä»¥ stackï¼‰
        return torch.stack(views, dim=1)  # [B, 2+num_local_crops, 3, img_size, img_size]
    
    return apply


# ============================================================
# 2. åŸºç¡€ resize + to_tensorï¼ˆä¸ normalizeï¼Œnormalize åœ¨å¢å¼ºååšï¼‰
# ============================================================

def ms_transform(img_size=96):
    """
    åŸºç¡€å˜æ¢ï¼šresize + to_tensorï¼Œä¸ normalize
    
    âœ… è¯´æ˜ï¼šå¦‚æœåŸå§‹æ•°æ®æ˜¯ 96Ã—96ï¼Œè€Œ img_size=112ï¼Œä¼šè‡ªåŠ¨ä¸Šé‡‡æ ·åˆ° 112Ã—112
    - v2.Resize(img_size) ä¼šå°†å›¾åƒ resize åˆ°æŒ‡å®šå°ºå¯¸ï¼ˆä¸Šé‡‡æ ·æˆ–ä¸‹é‡‡æ ·ï¼‰
    - v2.CenterCrop(img_size) ç¡®ä¿è¾“å‡ºå°ºå¯¸ä¸º img_sizeÃ—img_size
    - è¿™æ · 112 å¯ä»¥è¢« patch_size=14 æ•´é™¤ï¼Œpatch grid æ˜¯ 8Ã—8ï¼Œæ²¡æœ‰åƒç´ ä¸¢å¤±
    """
    return v2.Compose([
        v2.Resize(img_size),  # å¦‚æœåŸå§‹æ˜¯ 96Ã—96ï¼Œimg_size=112ï¼Œä¼šä¸Šé‡‡æ ·åˆ° 112Ã—112
        v2.CenterCrop(img_size),  # ç¡®ä¿è¾“å‡ºæ˜¯ img_sizeÃ—img_size
        v2.ToTensor(),  # è½¬æ¢ä¸º [0, 1] èŒƒå›´çš„ tensor
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œ normalizeï¼Œnormalize åœ¨å¢å¼ºååš
    ])


# ============================================================
# 3. Datasetï¼ˆè¯»å–å›¾ç‰‡ + åŸºç¡€å˜æ¢ï¼‰
# ============================================================

class DINODatasetGPU(Dataset):

    def __init__(self, base_ds, base_transform):
        self.ds = base_ds
        self.base_transform = base_transform

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        sample = self.ds[idx]
        img = sample["image"]

        if getattr(img, "mode", None) != "RGB":
            img = img.convert("RGB")

        x = self.base_transform(img)   # [3,H,W] CPU Tensor
        return x


# ============================================================
# 4. collate_fnï¼ˆæ‰¹é‡ CPU å †å ï¼ŒGPU å¢å¼ºåœ¨ä¸»è¿›ç¨‹è¿›è¡Œï¼‰
# ============================================================

def collate_fn_cpu(batch):
    """
    åœ¨ DataLoader worker ä¸­åªåš CPU æ“ä½œï¼Œé¿å… CUDA å¤šè¿›ç¨‹é—®é¢˜
    GPU å¢å¼ºå°†åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œ
    """
    # åœ¨ CPU ä¸Šå †å  batch
    x = torch.stack(batch, dim=0)  # [B, 3, H, W] CPU
    return x


# ============================================================
# 5. DataLoader ä¸»å‡½æ•°
# ============================================================

import os
import time
from glob import glob
from torch.utils.data import Dataset, DataLoader, Subset
from PIL import Image
import torch
import random

# =============================
# æœ¬åœ°æ–‡ä»¶ Datasetï¼ˆæ”¯æŒé€’å½’æœç´¢å­ç›®å½•ï¼‰
# =============================
class LocalImageDataset(Dataset):
    def __init__(self, root_dir, base_transform, recursive=True):
        """
        Args:
            root_dir: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
            base_transform: åŸºç¡€å˜æ¢
            recursive: æ˜¯å¦é€’å½’æœç´¢å­ç›®å½•ï¼ˆé»˜è®¤ Trueï¼‰
        """
        self.paths = []
        self.base_transform = base_transform
        
        # æ”¯æŒçš„å›¾ç‰‡æ ¼å¼
        extensions = ["jpg", "jpeg", "png", "JPG", "JPEG", "PNG"]
        
        if recursive:
            # é€’å½’æœç´¢æ‰€æœ‰å­ç›®å½•
            for ext in extensions:
                pattern = os.path.join(root_dir, "**", f"*.{ext}")
                self.paths.extend(glob(pattern, recursive=True))
        else:
            # åªæœç´¢æ ¹ç›®å½•
            for ext in extensions:
                pattern = os.path.join(root_dir, f"*.{ext}")
                self.paths.extend(glob(pattern, recursive=False))
        
        self.paths.sort()
        
        if len(self.paths) == 0:
            raise ValueError(
                f"æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶ï¼\n"
                f"  æœç´¢è·¯å¾„: {root_dir}\n"
                f"  é€’å½’æœç´¢: {recursive}\n"
                f"  æ”¯æŒçš„æ ¼å¼: {extensions}\n"
                f"  è¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å›¾ç‰‡æ˜¯å¦åœ¨å­ç›®å½•ä¸­ï¼ˆä½¿ç”¨ recursive=Trueï¼‰"
            )

    def __len__(self):
        return len(self.paths)

    def __getitem__(self, idx):
        path = self.paths[idx]
        img = Image.open(path).convert("RGB")
        x = self.base_transform(img)   # [3,H,W]
        return x


# =============================
# é«˜æ€§èƒ½æœ¬åœ° DataLoader
# =============================
def load_dino_data(
    dataset_type="local",
    dataset_name=None,
    dataset_root=None,
    img_size=96,
    batch_size=64,
    num_workers=8,
    train_sample=None,
    strength="strong",
    method="simclr",  # æ–°å¢ï¼šæ–¹æ³•ç±»å‹ï¼Œç”¨äºé€‰æ‹©æ­£ç¡®çš„å¢å¼ºç­–ç•¥
    num_local_crops=0,  # æ–°å¢ï¼šlocal crops æ•°é‡ï¼ˆDINOv2 ä½¿ç”¨ï¼‰
):
    """
    åŠ è½½æ•°æ®é›†ï¼ˆæ”¯æŒ HuggingFace å’Œæœ¬åœ°æ–‡ä»¶ï¼‰
    
    Args:
        dataset_type: "huggingface" æˆ– "local"
        dataset_name: HuggingFace æ•°æ®é›†åç§°ï¼ˆdataset_type="huggingface" æ—¶ä½¿ç”¨ï¼‰
        dataset_root: æœ¬åœ°å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆdataset_type="local" æ—¶ä½¿ç”¨ï¼‰
        img_size: å›¾åƒå°ºå¯¸
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
        train_sample: è®­ç»ƒé›†å­é›†å¤§å°
        strength: å¢å¼ºå¼ºåº¦
    
    Returns:
        train_loader, eval_loader, base_transform, two_view_aug
    """
    print("=" * 60)
    print("âš¡ Loading dataset with GPU-optimized pipeline")
    print("=" * 60)

    rng = random.Random(42)

    # --------------------------------------------------------
    # è¯»å–æ•°æ®é›†
    # --------------------------------------------------------
    if dataset_type == "huggingface":
        if dataset_name is None:
            dataset_name = "tsbpp/fall2025_deeplearning"  # é»˜è®¤å€¼
        from datasets import load_dataset as hf_load_dataset
        dataset = hf_load_dataset(dataset_name)
        base_train = dataset["train"]
        print(f"âœ” HF dataset loaded: {len(base_train)} images")
        
        # HuggingFace datasets é»˜è®¤è¿”å› PIL Imageï¼Œæ— éœ€é¢å¤–è®¾ç½®æ ¼å¼
        
        # ä½¿ç”¨åŸæ¥çš„ DINODatasetGPU
        base_transform = ms_transform(img_size)
        train_dataset = DINODatasetGPU(base_train, base_transform)
        
    elif dataset_type == "local":
        if dataset_root is None:
            raise ValueError("dataset_root must be provided when dataset_type='local'")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(dataset_root):
            raise ValueError(f"æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_root}")
        if not os.path.isdir(dataset_root):
            raise ValueError(f"è·¯å¾„ä¸æ˜¯ç›®å½•: {dataset_root}")
        
        print(f"âœ” Loading local dataset from: {dataset_root}")
        base_transform = ms_transform(img_size)
        
        # å°è¯•é€’å½’æœç´¢ï¼ˆé»˜è®¤ï¼‰
        try:
            full_dataset = LocalImageDataset(dataset_root, base_transform, recursive=True)
        except ValueError as e:
            # å¦‚æœé€’å½’æœç´¢å¤±è´¥ï¼Œå°è¯•éé€’å½’
            print(f"âš ï¸  é€’å½’æœç´¢å¤±è´¥ï¼Œå°è¯•éé€’å½’æœç´¢...")
            full_dataset = LocalImageDataset(dataset_root, base_transform, recursive=False)
        
        train_dataset = full_dataset
        print(f"âœ” Found {len(full_dataset)} local images")
        
    elif dataset_type == "cifar10":
        from torchvision.datasets import CIFAR10
        base_train = CIFAR10("./data", train=True, download=True)
        print(f"âœ” CIFAR10 loaded: {len(base_train)} images")
        base_transform = ms_transform(img_size)
        train_dataset = DINODatasetGPU(base_train, base_transform)
        
    elif dataset_type == "cifar100":
        from torchvision.datasets import CIFAR100
        base_train = CIFAR100("./data", train=True, download=True)
        print(f"âœ” CIFAR100 loaded: {len(base_train)} images")
        base_transform = ms_transform(img_size)
        train_dataset = DINODatasetGPU(base_train, base_transform)
        
    else:
        raise ValueError(f"Unknown dataset_type: {dataset_type}")

    # --------------------------------------------------------
    # train_sample é€»è¾‘
    # --------------------------------------------------------
    if len(train_dataset) == 0:
        raise ValueError(
            f"æ•°æ®é›†ä¸ºç©ºï¼æ‰¾ä¸åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶ã€‚\n"
            f"  è¯·æ£€æŸ¥:\n"
            f"  1. è·¯å¾„æ˜¯å¦æ­£ç¡®: {dataset_root if dataset_type == 'local' else dataset_name}\n"
            f"  2. å›¾ç‰‡æ ¼å¼æ˜¯å¦æ”¯æŒ: jpg, jpeg, png\n"
            f"  3. å¦‚æœå›¾ç‰‡åœ¨å­ç›®å½•ä¸­ï¼Œç¡®ä¿ä½¿ç”¨é€’å½’æœç´¢"
        )
    
    if train_sample is not None:
        train_sample = int(train_sample)
        total = len(train_dataset)
        use_n = min(train_sample, total)
        idxs = rng.sample(range(total), use_n)
        train_dataset = Subset(train_dataset, idxs)
        print(f"âœ” Using train subset: {use_n}/{total} images")
    else:
        print(f"âœ” Using full train set: {len(train_dataset)} images")

    # --------------------------------------------------------
    # æ„å»º transform å’Œ augment
    # --------------------------------------------------------
    two_view_aug = build_two_view_augment(
        img_size, 
        strength,
        method=method,
        num_local_crops=num_local_crops
    )

    # --------------------------------------------------------
    # åˆ›å»º DataLoaderï¼ˆGPU augment + é«˜é€Ÿçº¿ç¨‹ï¼‰
    # --------------------------------------------------------
    # ä¼˜åŒ–é…ç½®ï¼šå¹³è¡¡ prefetch_factor å’Œ persistent_workers
    if num_workers <= 8:
        prefetch_factor = 4
        use_persistent_workers = True
    elif num_workers <= 16:
        prefetch_factor = 3
        use_persistent_workers = True
    else: # num_workers > 16
        prefetch_factor = 2
        use_persistent_workers = False # å¤ªå¤šworkerå¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜ï¼Œç¦ç”¨persistent_workers
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        prefetch_factor=prefetch_factor if num_workers > 0 else None,
        persistent_workers=use_persistent_workers,
        drop_last=True,
        collate_fn=collate_fn_cpu,
    )
    
    print(f"âœ” DataLoader é…ç½®: num_workers={num_workers}, prefetch_factor={prefetch_factor}, persistent_workers={use_persistent_workers}")

    print(f"âœ” Train loader created: {len(train_loader)} batches")
    print("=" * 60)

    return train_loader, None, base_transform, two_view_aug




# ============================================================
# 6. æµ‹è¯•
# ============================================================

if __name__ == "__main__":
    loader, _, _, _ = load_dino_data(
        img_size=96,
        batch_size=128,
        num_workers=8,
        train_sample=50000,   # æµ‹è¯• sample
        strength="strong"
    )

    for batch in loader:
        print("Batch:", batch.shape)  # [B,2,3,96,96]
        break
