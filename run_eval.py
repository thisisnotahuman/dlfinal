"""
åœ¨ CUB-200-2011 ä¸Šè¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import pandas as pd
from pathlib import Path
import argparse

from model import build_method
from eval import extract_features, knn_eval, linear_probe_eval


class CUBDataset(Dataset):
    """CUB-200-2011 æ•°æ®é›†"""
    
    def __init__(self, image_dir, labels_csv=None, transform=None):
        """
        Args:
            image_dir: å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
            labels_csv: æ ‡ç­¾CSVæ–‡ä»¶ï¼ˆtrain/valæœ‰ï¼Œtestæ²¡æœ‰ï¼‰
            transform: å›¾åƒå˜æ¢
        """
        self.image_dir = Path(image_dir)
        self.transform = transform
        
        # åŠ è½½æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
        if labels_csv is not None:
            self.labels_df = pd.read_csv(labels_csv)
            self.has_labels = True
        else:
            # Test set: åªæœ‰å›¾ç‰‡åˆ—è¡¨
            self.labels_df = pd.DataFrame({
                'filename': [f.name for f in self.image_dir.glob('*.jpg')]
            })
            self.has_labels = False
        
        print(f"âœ” Loaded {len(self.labels_df)} images from {image_dir}")
    
    def __len__(self):
        return len(self.labels_df)
    
    def __getitem__(self, idx):
        row = self.labels_df.iloc[idx]
        
        # åŠ è½½å›¾ç‰‡
        img_path = self.image_dir / row['filename']
        img = Image.open(img_path).convert('RGB')
        
        if self.transform:
            img = self.transform(img)
        
        if self.has_labels:
            label = row['class_id']
            return img, label
        else:
            return img


def load_cub_data(data_dir, img_size=96, batch_size=256, num_workers=4):
    """
    åŠ è½½ CUB-200-2011 è¯„ä¼°æ•°æ®
    
    Args:
        data_dir: kaggle_data/ æ–‡ä»¶å¤¹è·¯å¾„
        
    Returns:
        train_loader: è®­ç»ƒé›†ï¼ˆç”¨äº k-NN feature bankï¼‰
        val_loader: éªŒè¯é›†ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    """
    data_dir = Path(data_dir)
    
    # å›¾åƒå˜æ¢ï¼ˆä¸åšå¢å¼ºï¼‰
    transform = transforms.Compose([
        transforms.Resize(img_size),
        transforms.CenterCrop(img_size),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = CUBDataset(
        image_dir=data_dir / 'train',
        labels_csv=data_dir / 'train_labels.csv',
        transform=transform
    )
    
    val_dataset = CUBDataset(
        image_dir=data_dir / 'val',
        labels_csv=data_dir / 'val_labels.csv',
        transform=transform
    )
    
    # åˆ›å»º DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True
    )
    
    return train_loader, val_loader


def main():
    parser = argparse.ArgumentParser("åœ¨ CUB-200-2011 ä¸Šè¯„ä¼°é¢„è®­ç»ƒæ¨¡å‹")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--checkpoint", type=str, required=True,
                       help="é¢„è®­ç»ƒæ¨¡å‹çš„ checkpoint è·¯å¾„")
    parser.add_argument("--method", type=str, default="simclr")
    parser.add_argument("--backbone_type", type=str, default="resnet50")
    
    # æ•°æ®å‚æ•°
    parser.add_argument("--data_dir", type=str, default="./data",
                       help="CUB æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆåŒ…å« train/val/testï¼‰")
    parser.add_argument("--img_size", type=int, default=96)
    parser.add_argument("--batch_size", type=int, default=256)
    parser.add_argument("--num_workers", type=int, default=4)
    
    # è¯„ä¼°å‚æ•°
    parser.add_argument("--eval_method", type=str, default="knn",
                       choices=["knn", "linear_probe"])
    parser.add_argument("--knn_k", type=int, default=20)
    parser.add_argument("--linear_probe_C", type=float, default=1.0)
    parser.add_argument("--use_cls_token", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨ CLS tokenï¼ˆä»… ViTï¼‰")
    
    args = parser.parse_args()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ è®¾å¤‡: {device}")
    
    # åŠ è½½è¯„ä¼°æ•°æ®
    print("\n" + "="*60)
    print("ğŸ“Š åŠ è½½ CUB-200-2011 æ•°æ®é›†...")
    print("="*60)
    
    train_loader, val_loader = load_cub_data(
        args.data_dir,
        args.img_size,
        args.batch_size,
        args.num_workers
    )
    
    # æ„å»ºæ–¹æ³•
    method_config = {
        "proj_hidden_dim": 2048,
        "proj_output_dim": 128,
        "temperature": 0.5,
    }
    
    method = build_method(
        method_name=args.method,
        backbone_type=args.backbone_type,
        pretrained_backbone=False,
        config=method_config
    ).to(device)
    
    # åŠ è½½ checkpoint
    print(f"\nğŸ“¥ åŠ è½½ checkpoint: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    method.load_state_dict(checkpoint["model_state_dict"])
    print(f"âœ” å·²åŠ è½½ epoch {checkpoint.get('epoch', '?')}, loss={checkpoint.get('avg_loss', '?'):.4f}")
    
    # è·å–ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰
    encoder = method.get_encoder()
    encoder.eval()
    
    # æå–ç‰¹å¾
    print("\n" + "="*60)
    print("ğŸ” æå–ç‰¹å¾...")
    print("="*60)
    
    print("\n1. æå–è®­ç»ƒé›†ç‰¹å¾ï¼ˆç”¨äº feature bankï¼‰...")
    train_features, train_labels = extract_features(
        encoder, train_loader, device, args.use_cls_token
    )
    print(f"   è®­ç»ƒé›†ç‰¹å¾: {train_features.shape}")
    
    print("\n2. æå–éªŒè¯é›†ç‰¹å¾...")
    val_features, val_labels = extract_features(
        encoder, val_loader, device, args.use_cls_token
    )
    print(f"   éªŒè¯é›†ç‰¹å¾: {val_features.shape}")
    
    # è¯„ä¼°
    print("\n" + "="*60)
    print(f"ğŸ“ˆ {args.eval_method.upper()} è¯„ä¼°...")
    print("="*60)
    
    if args.eval_method == "knn":
        accuracy = knn_eval(
            train_features, train_labels,
            val_features, val_labels,
            k=args.knn_k
        )
    elif args.eval_method == "linear_probe":
        accuracy = linear_probe_eval(
            train_features, train_labels,
            val_features, val_labels,
            C=args.linear_probe_C
        )
    
    # æ‰“å°ç»“æœ
    print("\n" + "="*60)
    print("ğŸ¯ æœ€ç»ˆç»“æœ")
    print("="*60)
    print(f"æ–¹æ³•: {args.eval_method}")
    print(f"Checkpoint: {args.checkpoint}")
    print(f"éªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print("="*60)


if __name__ == "__main__":
    main()