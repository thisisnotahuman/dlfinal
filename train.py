"""
é€šç”¨è®­ç»ƒå¾ªç¯ - å®Œå…¨å¯ä»¥å…±ç”¨çš„éƒ¨åˆ†
==================================================
ç»Ÿä¸€çš„è®­ç»ƒå¾ªç¯å¤–å£³ï¼Œæ¯ä¸ªæ–¹æ³•åªéœ€è¦å®ç° compute_loss()
"""

import os
import argparse
from datetime import datetime

import torch
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

from load_data import load_dino_data
from model import build_method
from utils import (
    build_optimizer,
    build_scheduler,
    save_checkpoint,
    count_parameters
)


# ============================================================
# é€šç”¨è®­ç»ƒå¾ªç¯
# ============================================================

def train_ssl(
    method,
    train_loader,
    device,
    optimizer,
    scheduler,
    epochs,
    save_dir,
    use_amp=True,
    save_freq=1,
    log_freq=100,
    two_view_aug=None  # å¢å¼ºå‡½æ•°
):
    """
    é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒå¾ªç¯
    
    Args:
        method: è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å®ä¾‹ï¼ˆç»§æ‰¿ BaseSSLMethodï¼‰
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•
        use_amp: æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        save_freq: ä¿å­˜é¢‘ç‡ï¼ˆæ¯ N ä¸ª epochï¼‰
        log_freq: æ—¥å¿—é¢‘ç‡ï¼ˆæ¯ N ä¸ª stepï¼‰
    """
    os.makedirs(save_dir, exist_ok=True)
    
    scaler = GradScaler() if use_amp else None
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   æ–¹æ³•: {method.__class__.__name__}")
    print(f"   å‚æ•°é‡: {count_parameters(method):,}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   AMP: {use_amp}")
    print()
    
    global_step = 0
    best_loss = float("inf")
    
    # Epoch å¾ªç¯
    for epoch in range(1, epochs + 1):
        method.train()
        epoch_loss = 0
        num_batches = 0
        
        # è¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}", ncols=100)
        
        for batch in pbar:
            # batch æ˜¯ [B, 3, H, W] CPU tensorï¼Œéœ€è¦ç§»åˆ° GPU å¹¶è¿›è¡Œå¢å¼º
            batch = batch.to(device, non_blocking=True)  # [B, 3, H, W] GPU
            
            # åº”ç”¨å¢å¼ºï¼ˆç”Ÿæˆ viewsï¼‰- åœ¨ä¸»è¿›ç¨‹çš„ GPU ä¸Šè¿›è¡Œ
            if two_view_aug is not None:
                batch = two_view_aug(batch)  # [B, 2, 3, H, W] GPU
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
            if use_amp:
                with autocast():
                    views = method.get_views(batch)
                    loss, loss_dict = method.compute_loss(views)
                
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                views = method.get_views(batch)
                loss, loss_dict = method.compute_loss(views)
                loss.backward()
                optimizer.step()
            
            # æ›´æ–° EMAï¼ˆå¦‚æœæœ‰ teacher ç½‘ç»œï¼‰
            method.update_ema()
            
            # æ›´æ–°å­¦ä¹ ç‡ï¼ˆæŸäº›æ–¹æ³•å¯èƒ½éœ€è¦åœ¨ step çº§åˆ«æ›´æ–°ï¼‰
            if hasattr(scheduler, 'step') and callable(getattr(scheduler, 'step', None)):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ epoch çº§åˆ«çš„ scheduler
                if not hasattr(scheduler, 'current_epoch'):
                    # å¦‚æœæ˜¯ step çº§åˆ«çš„ï¼Œåœ¨è¿™é‡Œæ›´æ–°
                    pass  # æš‚æ—¶ä¸åœ¨è¿™é‡Œæ›´æ–°ï¼Œåœ¨ epoch ç»“æŸåæ›´æ–°
            
            epoch_loss += loss.item()
            num_batches += 1
            global_step += 1
            
            # æ—¥å¿—
            if global_step % log_freq == 0:
                pbar.set_postfix({**loss_dict, "lr": f"{optimizer.param_groups[0]['lr']:.2e}"})
        
        # Epoch ç»“æŸï¼šæ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            scheduler.step()
        
        # è®¾ç½® epochï¼ˆç”¨äº DINO/iBOT çš„ warmupï¼‰
        if hasattr(method, 'set_epoch'):
            method.set_epoch(epoch)
        
        avg_loss = epoch_loss / max(1, num_batches)
        current_lr = scheduler.get_last_lr()[0] if scheduler is not None else optimizer.param_groups[0]['lr']
        
        print(f"\nğŸ“Œ Epoch {epoch}/{epochs}:")
        print(f"   avg_loss = {avg_loss:.4f}")
        print(f"   lr = {current_lr:.3e}")
        
        # ä¿å­˜ checkpoint
        # ä¿å­˜ checkpoint
        if epoch % save_freq == 0 or epoch == epochs:
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
            }
            
            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict() 
                    if hasattr(scheduler, 'scheduler') 
                    else scheduler.state_dict()
                )
            
            save_path = os.path.join(save_dir, f"epoch_{epoch:03d}.pth")
            # â¬…ï¸ ç›´æ¥ä¿å­˜ ckptï¼Œä¸å†ä¼  epoch, method, optimizer, scheduler
            torch.save(ckpt, save_path)
            print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {save_path}")

        # ä¿å­˜ best æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = os.path.join(save_dir, "best.pth")
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
            }
            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict() 
                    if hasattr(scheduler, 'scheduler') 
                    else scheduler.state_dict()
                )
            torch.save(ckpt, best_path)
            print(f"ğŸ… æ›´æ–° Best æ¨¡å‹ï¼ˆloss={best_loss:.4f}ï¼‰")


# ============================================================
# ä¸»è®­ç»ƒå‡½æ•°
# ============================================================

def main_train(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    train_loader, _, _, two_view_aug = load_dino_data(
        dataset_name=args.dataset_name,
        dataset_type=args.dataset_type,
        img_size=args.img_size,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        train_sample=args.train_sample,
        eval_samples=None,
        strength=args.aug_strength,
    )
    
    # å°† two_view_aug å­˜å‚¨ä¸ºå…¨å±€å˜é‡æˆ–ä¼ é€’ç»™è®­ç»ƒå‡½æ•°
    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä¿®æ”¹è®­ç»ƒå¾ªç¯æ¥ä½¿ç”¨å®ƒ
    
    # æ„å»ºæ–¹æ³•é…ç½®
    method_config = {
        "proj_hidden_dim": args.proj_hidden_dim,
        "proj_output_dim": args.proj_output_dim,
        "temperature": args.temperature,
        # å¯ä»¥æ·»åŠ å…¶ä»–æ–¹æ³•ç‰¹å®šçš„é…ç½®
    }
    
    # æ„å»ºæ–¹æ³•
    method = build_method(
        method_name=args.method,
        backbone_type=args.backbone_type,
        pretrained_backbone=args.pretrained_backbone,
        config=method_config
    ).to(device)
    
    # æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = build_optimizer(
        method,
        optimizer_type=args.optimizer_type,
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    scheduler = build_scheduler(
        optimizer,
        scheduler_type=args.scheduler_type,
        T_max=args.epochs,
        warmup_epochs=args.warmup_epochs
    )
    
    # è®­ç»ƒ
    train_ssl(
        method=method,
        train_loader=train_loader,
        device=device,
        optimizer=optimizer,
        scheduler=scheduler,
        epochs=args.epochs,
        save_dir=args.save_dir,
        use_amp=args.use_amp,
        save_freq=args.save_freq,
        log_freq=args.log_freq,
        two_view_aug=two_view_aug  # ä¼ é€’å¢å¼ºå‡½æ•°
    )


# ============================================================
# Argument Parser
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")
    
    # æ–¹æ³•é€‰æ‹©
    parser.add_argument("--method", type=str, default="simclr",
                       choices=["simclr", "moco", "byol", "dino", "ibot", "vicreg", "mae"],
                       help="è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•")
    
    # æ•°æ®
    parser.add_argument("--dataset_type", type=str, default="huggingface")
    parser.add_argument("--dataset_name", type=str, default="tsbpp/fall2025_deeplearning")
    parser.add_argument("--img_size", type=int, default=96)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument("--train_sample", type=int, default=None,
                       help="è®­ç»ƒé›†å­é›†å¤§å°ï¼Œä¾‹å¦‚ 50000")
    parser.add_argument("--aug_strength", type=str, default="strong",
                       choices=["strong", "weak"])
    
    # æ¨¡å‹
    parser.add_argument("--backbone_type", type=str, default="resnet50",
                       choices=["resnet50", "vit_b_16"])
    parser.add_argument("--pretrained_backbone", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒ backbone")
    
    # æ–¹æ³•ç‰¹å®šå‚æ•°ï¼ˆSimCLRï¼‰
    parser.add_argument("--proj_hidden_dim", type=int, default=2048)
    parser.add_argument("--proj_output_dim", type=int, default=128)
    parser.add_argument("--temperature", type=float, default=0.5)
    
    # è®­ç»ƒ
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--optimizer_type", type=str, default="adamw",
                       choices=["adamw", "sgd"])
    parser.add_argument("--scheduler_type", type=str, default="cosine",
                       choices=["cosine", "step"])
    parser.add_argument("--warmup_epochs", type=int, default=0)
    parser.add_argument("--use_amp", action="store_true", default=True,
                       help="ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦")
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--save_dir", type=str, default="./checkpoints")
    parser.add_argument("--save_freq", type=int, default=1,
                       help="æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡")
    parser.add_argument("--log_freq", type=int, default=100,
                       help="æ¯ N ä¸ª step è®°å½•ä¸€æ¬¡æ—¥å¿—")
    
    return parser.parse_args()


# ============================================================
# Main
# ============================================================

def main():
    torch.backends.cudnn.benchmark = True
    print("=" * 60)
    print("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now()}")
    print("=" * 60)
    
    args = parse_args()
    main_train(args)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
