"""
é€šç”¨è®­ç»ƒå¾ªç¯ - å®Œå…¨å¯ä»¥å…±ç”¨çš„éƒ¨åˆ†
==================================================
ç»Ÿä¸€çš„è®­ç»ƒå¾ªç¯å¤–å£³ï¼Œæ¯ä¸ªæ–¹æ³•åªéœ€è¦å®ç° compute_loss()
"""

import os
import argparse
import time
from datetime import datetime

import torch
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import wandb

from load_data import load_dino_data
from model import build_method
from utils import (
    build_optimizer,
    build_scheduler,
    save_checkpoint,  # ç›®å‰æ²¡ç”¨åˆ°ï¼Œä½†å…ˆç•™ç€
    count_parameters
)
from eval import evaluate_on_cub


# ============================================================
# é€šç”¨è®­ç»ƒå¾ªç¯
# ============================================================

def train_ssl(
    method,
    train_loader,
    device,
    optimizer,
    scheduler,
    epochs,
    save_dir,
    two_view_aug,
    use_amp=True,
    save_freq=None,
    log_freq=100,
    use_wandb=False,
    wandb_project="ssl-pretraining",
    wandb_name=None,
    early_stop_patience=None,
    early_stop_min_delta=0.0001,
    # æ¢å¤è®­ç»ƒå‚æ•°
    start_epoch=1,  # ä»å“ªä¸ª epoch å¼€å§‹ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
    start_global_step=0,  # ä»å“ªä¸ª global_step å¼€å§‹ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
    start_best_loss=float("inf"),  # åˆå§‹ best_lossï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
    # è¯„ä¼°ç›¸å…³å‚æ•°
    eval_enabled=False,
    eval_cub_data_dir=None,
    eval_freq=2,  # æ¯ N ä¸ª epoch è¯„ä¼°ä¸€æ¬¡
    eval_method="knn",
    eval_knn_k=20,
    eval_linear_probe_C=1.0,
    eval_use_cls_token=False,
    eval_batch_size=256,
    eval_num_workers=4,
    img_size=96,  # å›¾åƒå°ºå¯¸ï¼ˆç”¨äºè¯„ä¼°ï¼‰
    disable_tqdm=False,  # æ˜¯å¦ç¦ç”¨ tqdm è¿›åº¦æ¡
):
    """
    é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒå¾ªç¯
    
    Args:
        method: è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å®ä¾‹ï¼ˆç»§æ‰¿ BaseSSLMethodï¼‰
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•
        two_view_aug: æ•°æ®å¢å¼ºå‡½æ•°ï¼ˆè¾“å…¥ä¸€æ‰¹å›¾åƒï¼Œè¾“å‡ºä¸¤ç»„å¢å¼ºè§†å›¾ï¼‰
        use_amp: æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        save_freq: ä¿å­˜é¢‘ç‡ï¼ˆæ¯ N ä¸ª epochï¼‰
        log_freq: æ—¥å¿—é¢‘ç‡ï¼ˆæ¯ N ä¸ª stepï¼‰
        use_wandb: æ˜¯å¦ä½¿ç”¨ wandb ç›‘æ§
        wandb_project: wandb é¡¹ç›®å
        wandb_name: wandb è¿è¡Œåç§°
        early_stop_patience: æ—©åœè€å¿ƒå€¼ï¼ˆè¿ç»­å¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„åˆ™åœæ­¢ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸ä½¿ç”¨æ—©åœ
        early_stop_min_delta: æ—©åœæœ€å°æ”¹å–„é˜ˆå€¼
        eval_enabled: æ˜¯å¦å¯ç”¨è¯„ä¼°
        eval_cub_data_dir: CUB æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆeval_enabled=True æ—¶å¿…éœ€ï¼‰
        eval_freq: è¯„ä¼°é¢‘ç‡ï¼ˆæ¯ N ä¸ª epoch è¯„ä¼°ä¸€æ¬¡ï¼‰
        eval_method: è¯„ä¼°æ–¹æ³•ï¼Œ"knn" æˆ– "linear_probe"
        eval_knn_k: k-NN çš„ k å€¼
        eval_linear_probe_C: Linear Probe çš„æ­£åˆ™åŒ–å¼ºåº¦
        eval_use_cls_token: æ˜¯å¦ä½¿ç”¨ CLS tokenï¼ˆä»… ViTï¼‰
        eval_batch_size: è¯„ä¼°æ—¶çš„æ‰¹æ¬¡å¤§å°
        eval_num_workers: è¯„ä¼°æ—¶çš„æ•°æ®åŠ è½½çº¿ç¨‹æ•°
        img_size: å›¾åƒå°ºå¯¸ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        disable_tqdm: æ˜¯å¦ç¦ç”¨ tqdm è¿›åº¦æ¡
    """
    os.makedirs(save_dir, exist_ok=True)

    scaler = GradScaler() if use_amp else None

    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   æ–¹æ³•: {method.__class__.__name__}")
    print(f"   å‚æ•°é‡: {count_parameters(method):,}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   AMP: {use_amp}")
    if use_wandb:
        print(f"   Wandb: âœ… {wandb_project}/{wandb_name}")
    if early_stop_patience is not None:
        print(f"   æ—©åœ: âœ… patience={early_stop_patience}, min_delta={early_stop_min_delta}")
    if eval_enabled:
        print(f"   è¯„ä¼°: âœ… æ¯ {eval_freq} ä¸ª epoch åœ¨ CUB-200-2011 ä¸Šè¯„ä¼° ({eval_method})")
        print(f"         CUB æ•°æ®è·¯å¾„: {eval_cub_data_dir}")
    print()

    global_step = start_global_step  # âœ… æ¢å¤è®­ç»ƒï¼šä»æŒ‡å®š global_step å¼€å§‹
    best_loss = start_best_loss  # âœ… æ¢å¤è®­ç»ƒï¼šä»æŒ‡å®š best_loss å¼€å§‹
    epochs_without_improvement = 0  # æ—©åœè®¡æ•°å™¨

    # Epoch å¾ªç¯
    for epoch in range(start_epoch, epochs + 1):  # âœ… æ¢å¤è®­ç»ƒï¼šä»æŒ‡å®š epoch å¼€å§‹
        # âœ… æ·»åŠ ï¼šè®°å½• epoch å¼€å§‹æ—¶é—´
        epoch_start_time = time.time()
        
        method.train()
        epoch_loss = 0.0
        num_batches = 0

        # è¿›åº¦æ¡
        if disable_tqdm:
            pbar = train_loader
        else:
            pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}", ncols=100)

        for batch in pbar:
            # å°† batch ç§»åˆ°è®¾å¤‡å¹¶åšä¸¤è§†å›¾å¢å¼º
            if isinstance(batch, torch.Tensor):
                batch = batch.to(device, non_blocking=True)
                views = two_view_aug(batch)
            elif isinstance(batch, (list, tuple)):
                batch = [
                    b.to(device, non_blocking=True) if isinstance(b, torch.Tensor) else b
                    for b in batch
                ]
                views = method.get_views(batch)
            else:
                views = method.get_views(batch)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
            if use_amp:
                with autocast():
                    loss, loss_dict = method.compute_loss(views)

                # æ£€æŸ¥ loss æ˜¯å¦ä¸º NaN/Inf
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  Warning: NaN/Inf loss detected at step {global_step}, skipping this batch")
                    continue  # è·³è¿‡è¿™ä¸ª batch
                
                scaler.scale(loss).backward()
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰ inf/nanï¼ˆå¿…é¡»åœ¨ step ä¹‹å‰ï¼‰
                scaler.unscale_(optimizer)
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰ inf/nan
                grad_norm = torch.nn.utils.clip_grad_norm_(method.parameters(), max_norm=1.0)
                
                # å¦‚æœæ¢¯åº¦æœ‰ inf/nanï¼Œscaler ä¼šè·³è¿‡ step
                scaler.step(optimizer)
                scaler.update()
            else:
                loss, loss_dict = method.compute_loss(views)
                
                # æ£€æŸ¥ loss æ˜¯å¦ä¸º NaN/Inf
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f"âš ï¸  Warning: NaN/Inf loss detected at step {global_step}, skipping this batch")
                    continue  # è·³è¿‡è¿™ä¸ª batch
                
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(method.parameters(), max_norm=1.0)
                
                optimizer.step()

            # æ›´æ–° EMAï¼ˆå¦‚æœæœ‰ teacher ç½‘ç»œï¼‰
            method.update_ema()

            epoch_loss += loss.item()
            num_batches += 1
            global_step += 1
            
            # æ£€æŸ¥æ¢¯åº¦ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            if global_step % log_freq == 0 and not use_amp:
                # âœ… æ€§èƒ½ä¼˜åŒ–ï¼šåœ¨ GPU ä¸Šè®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼Œæœ€ååªè°ƒç”¨ä¸€æ¬¡ .item()
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆä»…åœ¨é AMP æ¨¡å¼ä¸‹ï¼Œé¿å…å½±å“æ€§èƒ½ï¼‰
                total_norm_sq = torch.tensor(0.0, device=device)
                for p in method.parameters():
                    if p.grad is not None:
                        param_norm_sq = p.grad.data.norm(2) ** 2
                        total_norm_sq = total_norm_sq + param_norm_sq
                total_norm = (total_norm_sq ** 0.5).item()  # åªåœ¨æœ€åè°ƒç”¨ä¸€æ¬¡ .item()
                if total_norm > 0:
                    loss_dict["grad_norm"] = total_norm

            # Step çº§åˆ«æ—¥å¿—
            if global_step % log_freq == 0:
                current_lr = optimizer.param_groups[0]["lr"]
                if not disable_tqdm:
                    pbar.set_postfix({**loss_dict, "lr": f"{current_lr:.2e}"})

                if use_wandb:
                    wandb.log(
                        {
                            "train/loss_step": loss.item(),
                            "train/lr": current_lr,
                            "train/epoch": epoch,
                            **{f"train/{k}": v for k, v in loss_dict.items()},
                        },
                        step=global_step,
                    )

        # Epoch ç»“æŸï¼šæ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            scheduler.step()

        # è®¾ç½® epochï¼ˆç”¨äº DINO/iBOT çš„ warmup ç­‰ï¼‰
        if hasattr(method, "set_epoch"):
            method.set_epoch(epoch)

        avg_loss = epoch_loss / max(1, num_batches)
        current_lr = (
            scheduler.get_last_lr()[0]
            if scheduler is not None
            else optimizer.param_groups[0]["lr"]
        )
        
        # âœ… æ·»åŠ ï¼šè®¡ç®— epoch è€—æ—¶
        epoch_time = time.time() - epoch_start_time
        epoch_time_min = epoch_time / 60.0
        epoch_time_sec = epoch_time % 60

        print(f"\nğŸ“Œ Epoch {epoch}/{epochs}:")
        print(f"   avg_loss = {avg_loss:.4f}")
        print(f"   lr = {current_lr:.3e}")
        print(f"   è€—æ—¶ = {int(epoch_time_min)}åˆ†{int(epoch_time_sec)}ç§’ ({epoch_time:.2f}ç§’)")

        # Epoch çº§åˆ«æ—¥å¿—
        if use_wandb:
            wandb.log(
                {
                    "train/loss_epoch": avg_loss,
                    "train/lr_epoch": current_lr,
                    "epoch": epoch,
                },
                step=global_step,
            )

        # ä¿å­˜å½“å‰ epoch çš„ checkpointï¼ˆä»…åœ¨ save_freq ä¸ä¸º None æ—¶ä¿å­˜ï¼‰
        if save_freq is not None and (epoch % save_freq == 0 or epoch == epochs):
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
                "best_loss": best_loss,  # âœ… æ·»åŠ ï¼šä¿å­˜ best_loss ä»¥ä¾¿æ¢å¤è®­ç»ƒ
            }

            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict()
                    if hasattr(scheduler, "scheduler")
                    else scheduler.state_dict()
                )

            save_path = os.path.join(save_dir, f"epoch_{epoch:03d}.pth")
            torch.save(ckpt, save_path)
            print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {save_path}")

            if use_wandb:
                wandb.save(save_path)
        
        # ä¿å­˜ latest checkpointï¼ˆæ¯ä¸ª epoch éƒ½ä¿å­˜ï¼‰
        latest_path = os.path.join(save_dir, "latest.pth")
        ckpt = {
            "epoch": epoch,
            "model_state_dict": method.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "avg_loss": avg_loss,
            "global_step": global_step,
            "best_loss": best_loss,  # âœ… æ·»åŠ ï¼šä¿å­˜ best_loss ä»¥ä¾¿æ¢å¤è®­ç»ƒ
        }
        if scheduler is not None:
            ckpt["scheduler_state_dict"] = (
                scheduler.scheduler.state_dict()
                if hasattr(scheduler, "scheduler")
                else scheduler.state_dict()
            )
        torch.save(ckpt, latest_path)
        if epoch == 1 or epoch % log_freq == 0:
            print(f"ğŸ’¾ æ›´æ–° Latest æ¨¡å‹åˆ° {latest_path}")

        # ä¿å­˜ best æ¨¡å‹ & æ—©åœé€»è¾‘
        if avg_loss < best_loss - early_stop_min_delta:
            best_loss = avg_loss
            epochs_without_improvement = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨

            best_path = os.path.join(save_dir, "best.pth")
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
                "best_loss": best_loss,  # âœ… æ·»åŠ ï¼šä¿å­˜ best_loss
            }
            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict()
                    if hasattr(scheduler, "scheduler")
                    else scheduler.state_dict()
                )
            torch.save(ckpt, best_path)
            print(f"ğŸ… æ›´æ–° Best æ¨¡å‹ï¼ˆloss={best_loss:.4f}ï¼‰åˆ° {best_path}")

            if use_wandb:
                wandb.run.summary["best_loss"] = best_loss
                wandb.run.summary["best_epoch"] = epoch
                wandb.save(best_path)
        else:
            epochs_without_improvement += 1
            if early_stop_patience is not None:
                print(f"âš ï¸  Loss æ²¡æœ‰æ”¹å–„ ({epochs_without_improvement}/{early_stop_patience})")

        # è¯„ä¼°ï¼ˆæ¯ eval_freq ä¸ª epochï¼‰
        if eval_enabled and epoch % eval_freq == 0:
            # âœ… æ·»åŠ ï¼šè®°å½•è¯„ä¼°å¼€å§‹æ—¶é—´
            eval_start_time = time.time()
            
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Epoch {epoch}: å¼€å§‹è¯„ä¼°...")
            print(f"{'='*60}")
            try:
                eval_results = evaluate_on_cub(
                    method=method,
                    cub_data_dir=eval_cub_data_dir,
                    device=device,
                    img_size=img_size,
                    batch_size=eval_batch_size,
                    num_workers=eval_num_workers,
                    eval_method=eval_method,
                    use_cls_token=eval_use_cls_token,
                    knn_k=eval_knn_k,
                    linear_probe_C=eval_linear_probe_C,
                    verbose=True,
                    disable_tqdm=disable_tqdm
                )
                
                # âœ… æ·»åŠ ï¼šè®¡ç®—è¯„ä¼°è€—æ—¶
                eval_time = time.time() - eval_start_time
                eval_time_min = eval_time / 60.0
                eval_time_sec = eval_time % 60
                
                eval_accuracy = eval_results["accuracy"]
                print(f"\nâœ… Epoch {epoch} è¯„ä¼°å®Œæˆ: {eval_method} accuracy = {eval_accuracy:.4f} ({eval_accuracy*100:.2f}%)")
                print(f"   è¯„ä¼°è€—æ—¶ = {int(eval_time_min)}åˆ†{int(eval_time_sec)}ç§’ ({eval_time:.2f}ç§’)")
                
                # è®°å½•åˆ° wandb
                if use_wandb:
                    wandb.log(
                        {
                            f"eval/{eval_method}_accuracy": eval_accuracy,
                            "epoch": epoch,
                        },
                        step=global_step,
                    )
            except Exception as e:
                print(f"âš ï¸  è¯„ä¼°å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        # æ—©åœæ£€æŸ¥
        if (
            early_stop_patience is not None
            and epochs_without_improvement >= early_stop_patience
        ):
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {early_stop_patience} ä¸ª epoch æ²¡æœ‰æ”¹å–„")
            print(f"   Best loss: {best_loss:.4f} (Epoch {epoch - early_stop_patience})")
            if use_wandb:
                wandb.run.summary["early_stopped"] = True
                wandb.run.summary["stopped_epoch"] = epoch
            break


# ============================================================
# ä¸»è®­ç»ƒå‡½æ•°
# ============================================================

def main_train(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # éªŒè¯è¯„ä¼°å‚æ•°
    if args.eval_enabled:
        if args.eval_cub_data_dir is None:
            raise ValueError("--eval_cub_data_dir å¿…é¡»æä¾›ï¼ˆå½“ --eval_enabled æ—¶ï¼‰")
        from pathlib import Path
        cub_path = Path(args.eval_cub_data_dir)
        if not cub_path.exists():
            raise ValueError(f"CUB æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.eval_cub_data_dir}")
        if not (cub_path / "train").exists() or not (cub_path / "val").exists():
            raise ValueError(f"CUB æ•°æ®è·¯å¾„æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”åŒ…å« train/ å’Œ val/ æ–‡ä»¶å¤¹: {args.eval_cub_data_dir}")
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ è®¾å¤‡: {device}")
    
    # ç¡®å®šä¿å­˜ç›®å½•
    print(f"ğŸ” è·¯å¾„å¤„ç†è°ƒè¯•ä¿¡æ¯:")
    print(f"   ä¼ å…¥çš„ --save_dir: {args.save_dir}")
    print(f"   ä¼ å…¥çš„ --exp_name: {args.exp_name}")
    print(f"   å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    if args.save_dir is None:
        if args.exp_name:
            args.save_dir = os.path.join("./checkpoints", args.exp_name)
        else:
            # å¦‚æœæ²¡æœ‰æä¾›exp_nameï¼Œä½¿ç”¨é»˜è®¤å‘½å
            args.save_dir = os.path.join(
                "./checkpoints",
                f"{args.method}_{args.backbone_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
    else:
        # å¦‚æœæä¾›äº† save_dirï¼Œä¸”ä¹Ÿæä¾›äº† exp_nameï¼Œåˆ™ç»„åˆè·¯å¾„
        if args.exp_name:
            args.save_dir = os.path.join(args.save_dir, args.exp_name)
            print(f"   ç»„åˆåçš„è·¯å¾„ï¼ˆæ‹¼æ¥åï¼‰: {args.save_dir}")
    
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„é—®é¢˜
    # å¦‚æœå·²ç»æ˜¯ç»å¯¹è·¯å¾„ï¼Œos.path.abspath() ä¼šä¿æŒä¸å˜
    original_save_dir = args.save_dir
    args.save_dir = os.path.abspath(args.save_dir)
    
    print(f"ğŸ“ Checkpoint ä¿å­˜ç›®å½•ï¼ˆæœ€ç»ˆï¼‰: {args.save_dir}")
    if original_save_dir != args.save_dir:
        print(f"   âš ï¸  è·¯å¾„å·²ä»ç›¸å¯¹è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„")
    print(f"   ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(args.save_dir)}")
    if not os.path.exists(args.save_dir):
        print(f"   âš ï¸  ç›®å½•ä¸å­˜åœ¨ï¼Œå°†åˆ›å»º: {args.save_dir}")

    # åˆå§‹åŒ– wandb
    if args.use_wandb:
        wandb_name = (
            args.wandb_name
            or f"{args.method}_{args.backbone_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        wandb.init(
            project=args.wandb_project,
            name=wandb_name,
            config={
                "method": args.method,
                "backbone_type": args.backbone_type,
                "img_size": args.img_size,
                "batch_size": args.batch_size,
                "epochs": args.epochs,
                "lr": args.lr,
                "weight_decay": args.weight_decay,
                "optimizer_type": args.optimizer_type,
                "scheduler_type": args.scheduler_type,
                "warmup_epochs": args.warmup_epochs,
                "temperature": args.temperature,
                "proj_hidden_dim": args.proj_hidden_dim,
                "proj_output_dim": args.proj_output_dim,
                "aug_strength": args.aug_strength,
                "train_sample": args.train_sample,
                "dataset_type": args.dataset_type,
                "dataset_root": args.dataset_root,
                "dataset_name": args.dataset_name,
            },
        )

    # åŠ è½½æ•°æ®ï¼ˆè¿™é‡Œåˆå¹¶æœ¬åœ° / HF å…¥å£ï¼‰
    # æ ¹æ®æ–¹æ³•ç±»å‹å†³å®šæ˜¯å¦ä½¿ç”¨ multi-crop
    use_multi_crop = args.method.lower() in ["dino", "dinov2", "ibot"]
    num_local_crops = 8 if use_multi_crop else 0  # DINOv2 é»˜è®¤ 8 ä¸ª local crops
    
    train_loader, _, _, two_view_aug = load_dino_data(
        dataset_type=args.dataset_type,     # "local" æˆ– "huggingface"
        dataset_root=args.dataset_root,     # æœ¬åœ°æ—¶ä½¿ç”¨
        dataset_name=args.dataset_name,     # HF æ—¶ä½¿ç”¨
        img_size=args.img_size,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        train_sample=args.train_sample,
        strength=args.aug_strength,
        method=args.method,  # ä¼ é€’æ–¹æ³•ç±»å‹
        num_local_crops=num_local_crops,  # ä¼ é€’ local crops æ•°é‡
    )

    # æ„å»ºæ–¹æ³•é…ç½®
    method_config = {
        "proj_hidden_dim": args.proj_hidden_dim,
        "proj_output_dim": args.proj_output_dim,
        "temperature": args.temperature,
        "img_size": args.img_size,  # ä¼ é€’ç»™ backbone æ„å»ºå‡½æ•°ï¼Œç”¨äº ViT çš„è‡ªå®šä¹‰å›¾åƒå°ºå¯¸
        "total_epochs": args.epochs,  # âœ… ä¿®å¤ï¼šä¼ é€’ç»™ DINOv2 ç”¨äº momentum cosine è°ƒåº¦
    }

    # æ„å»ºæ–¹æ³•
    method = build_method(
        method_name=args.method,
        backbone_type=args.backbone_type,
        pretrained_backbone=args.pretrained_backbone,
        config=method_config,
    ).to(device)

    # Wandb watch modelï¼ˆå¯é€‰ï¼‰
    if args.use_wandb and args.wandb_watch:
        wandb.watch(method, log="all", log_freq=args.log_freq)

    # æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = build_optimizer(
        method,
        optimizer_type=args.optimizer_type,
        lr=args.lr,
        weight_decay=args.weight_decay,
    )

    scheduler = build_scheduler(
        optimizer,
        scheduler_type=args.scheduler_type,
        T_max=args.epochs,
        warmup_epochs=args.warmup_epochs,
    )

    # ============================================================
    # æ¢å¤è®­ç»ƒï¼šä» checkpoint åŠ è½½
    # ============================================================
    start_epoch = 1
    global_step = 0
    best_loss = float("inf")
    
    if args.resume:
        print("\n" + "="*60)
        print(f"ğŸ”„ ä» checkpoint æ¢å¤è®­ç»ƒ: {args.resume}")
        print("="*60)
        
        if not os.path.exists(args.resume):
            raise FileNotFoundError(f"Checkpoint æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}")
        
        checkpoint = torch.load(args.resume, map_location=device)
        
        # åŠ è½½æ¨¡å‹
        method.load_state_dict(checkpoint["model_state_dict"])
        print("âœ… æ¨¡å‹æƒé‡å·²åŠ è½½")
        
        # åŠ è½½ä¼˜åŒ–å™¨
        if "optimizer_state_dict" in checkpoint:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
        
        # åŠ è½½è°ƒåº¦å™¨
        if "scheduler_state_dict" in checkpoint and scheduler is not None:
            try:
                if hasattr(scheduler, "scheduler"):
                    scheduler.scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
                else:
                    scheduler.load_state_dict(checkpoint["scheduler_state_dict"])
                print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€å·²åŠ è½½")
            except Exception as e:
                print(f"âš ï¸  è­¦å‘Šï¼šæ— æ³•åŠ è½½è°ƒåº¦å™¨çŠ¶æ€: {e}")
                print("   å°†ä½¿ç”¨æ–°çš„è°ƒåº¦å™¨çŠ¶æ€ç»§ç»­è®­ç»ƒ")
        
        # åŠ è½½è®­ç»ƒçŠ¶æ€
        if "epoch" in checkpoint:
            start_epoch = checkpoint["epoch"] + 1  # ä»ä¸‹ä¸€ä¸ª epoch å¼€å§‹
            print(f"âœ… ä» Epoch {start_epoch} å¼€å§‹è®­ç»ƒï¼ˆå·²è®­ç»ƒåˆ° Epoch {checkpoint['epoch']}ï¼‰")
        
        if "global_step" in checkpoint:
            global_step = checkpoint["global_step"]
            print(f"âœ… Global step: {global_step}")
        
        if "avg_loss" in checkpoint:
            print(f"âœ… ä¸Šä¸€ä¸ª epoch çš„å¹³å‡ loss: {checkpoint['avg_loss']:.4f}")
        
        if "best_loss" in checkpoint:
            best_loss = checkpoint["best_loss"]
            print(f"âœ… Best loss: {best_loss:.4f}")
        
        # å¦‚æœ DINOv2 æœ‰ teacher ç½‘ç»œï¼Œéœ€è¦ç¡®ä¿ teacher ä¹Ÿè¢«æ­£ç¡®åŠ è½½
        if hasattr(method, 'teacher_backbone') and hasattr(method, 'teacher_head'):
            print("âœ… DINOv2 teacher ç½‘ç»œå·²éšæ¨¡å‹ä¸€èµ·åŠ è½½")
        
        print("="*60)
        print()

    # ============================================================
    # è®­ç»ƒå‰æ£€æŸ¥ï¼šéªŒè¯è®­ç»ƒä»£ç æ˜¯å¦æ­£ç¡®
    # ============================================================
    print("\n" + "="*60)
    print("ğŸ” è®­ç»ƒå‰æ£€æŸ¥ï¼šéªŒè¯è®­ç»ƒä»£ç æ˜¯å¦æ­£ç¡®")
    print("="*60)
    
    # æ£€æŸ¥ä¼˜åŒ–å™¨ï¼šbackbone å‚æ•°æ˜¯å¦åœ¨ä¼˜åŒ–å™¨ä¸­
    optimizer_param_ids = set(id(p) for group in optimizer.param_groups for p in group['params'])
    backbone_param_ids = set(id(p) for p in method.backbone.parameters())
    head_param_ids = set(id(p) for p in method.head.parameters())
    
    backbone_in_optimizer = len(backbone_param_ids & optimizer_param_ids) > 0
    head_in_optimizer = len(head_param_ids & optimizer_param_ids) > 0
    
    print(f"ğŸ“Š ä¼˜åŒ–å™¨å‚æ•°æ£€æŸ¥ï¼š")
    print(f"   Backbone å‚æ•°åœ¨ä¼˜åŒ–å™¨ä¸­: {'âœ… æ˜¯' if backbone_in_optimizer else 'âŒ å¦ï¼ˆè¿™æ˜¯ä¸¥é‡é—®é¢˜ï¼ï¼‰'}")
    print(f"   Head å‚æ•°åœ¨ä¼˜åŒ–å™¨ä¸­: {'âœ… æ˜¯' if head_in_optimizer else 'âŒ å¦ï¼ˆè¿™æ˜¯ä¸¥é‡é—®é¢˜ï¼ï¼‰'}")
    
    if not backbone_in_optimizer:
        print("\nâš ï¸  ä¸¥é‡è­¦å‘Šï¼šBackbone å‚æ•°ä¸åœ¨ä¼˜åŒ–å™¨ä¸­ï¼Œä¸ä¼šè¢«æ›´æ–°ï¼")
        print("   è¿™ä¼šå¯¼è‡´è®­ç»ƒæ— æ•ˆï¼Œå‡†ç¡®ç‡ä¸ä¼šæå‡ï¼")
        print("   è¯·æ£€æŸ¥ä»£ç ï¼Œç¡®ä¿ backbone å‚æ•°è¢«æ·»åŠ åˆ°ä¼˜åŒ–å™¨ä¸­ã€‚")
    
    # æ£€æŸ¥æ¢¯åº¦ï¼šbackbone æ˜¯å¦æœ‰æ¢¯åº¦
    print(f"\nğŸ“Š æ¢¯åº¦æ£€æŸ¥ï¼š")
    method.train()
    dummy_batch = torch.randn(2, 3, args.img_size, args.img_size).to(device)
    views = torch.stack([dummy_batch, dummy_batch], dim=1)  # [2, 2, 3, H, W]
    
    optimizer.zero_grad()
    loss, _ = method.compute_loss(views)
    loss.backward()
    
    backbone_has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in method.backbone.parameters())
    head_has_grad = any(p.grad is not None and p.grad.abs().sum() > 0 for p in method.head.parameters())
    
    print(f"   Backbone æœ‰æ¢¯åº¦: {'âœ… æ˜¯' if backbone_has_grad else 'âŒ å¦ï¼ˆè¿™æ˜¯ä¸¥é‡é—®é¢˜ï¼ï¼‰'}")
    print(f"   Head æœ‰æ¢¯åº¦: {'âœ… æ˜¯' if head_has_grad else 'âŒ å¦ï¼ˆè¿™æ˜¯ä¸¥é‡é—®é¢˜ï¼ï¼‰'}")
    
    if not backbone_has_grad:
        print("\nâš ï¸  ä¸¥é‡è­¦å‘Šï¼šBackbone æ²¡æœ‰æ¢¯åº¦ï¼Œæ¢¯åº¦æ²¡æœ‰æ­£ç¡®ä¼ æ’­ï¼")
        print("   è¿™ä¼šå¯¼è‡´è®­ç»ƒæ— æ•ˆï¼Œå‡†ç¡®ç‡ä¸ä¼šæå‡ï¼")
        print("   è¯·æ£€æŸ¥ä»£ç ï¼Œç¡®ä¿æ¢¯åº¦èƒ½å¤Ÿä¼ æ’­åˆ° backboneã€‚")
    
    if backbone_in_optimizer and backbone_has_grad:
        print("\nâœ… è®­ç»ƒä»£ç æ£€æŸ¥é€šè¿‡ï¼šBackbone ä¼šè¢«æ­£ç¡®æ›´æ–°")
    else:
        print("\nâŒ è®­ç»ƒä»£ç æ£€æŸ¥å¤±è´¥ï¼šå­˜åœ¨é—®é¢˜ï¼Œéœ€è¦ä¿®å¤ï¼")
    
    print("="*60)
    print()

    # è®­ç»ƒ
    train_ssl(
        method=method,
        train_loader=train_loader,
        device=device,
        optimizer=optimizer,
        scheduler=scheduler,
        epochs=args.epochs,
        save_dir=args.save_dir,
        two_view_aug=two_view_aug,
        use_amp=args.use_amp,
        save_freq=args.save_freq,
        log_freq=args.log_freq,
        use_wandb=args.use_wandb,
        wandb_project=args.wandb_project,
        wandb_name=args.wandb_name,
        early_stop_patience=args.early_stop_patience,
        early_stop_min_delta=args.early_stop_min_delta,
        start_epoch=start_epoch,  # âœ… æ·»åŠ ï¼šä»æŒ‡å®š epoch å¼€å§‹
        start_global_step=global_step,  # âœ… æ·»åŠ ï¼šä»æŒ‡å®š global_step å¼€å§‹
        start_best_loss=best_loss,  # âœ… æ·»åŠ ï¼šä»æŒ‡å®š best_loss å¼€å§‹
        # è¯„ä¼°å‚æ•°
        eval_enabled=args.eval_enabled,
        eval_cub_data_dir=args.eval_cub_data_dir,
        eval_freq=args.eval_freq,
        eval_method=args.eval_method,
        eval_knn_k=args.eval_knn_k,
        eval_linear_probe_C=args.eval_linear_probe_C,
        eval_use_cls_token=args.eval_use_cls_token,
        eval_batch_size=args.eval_batch_size,
        eval_num_workers=args.eval_num_workers,
        img_size=args.img_size,
        disable_tqdm=args.disable_tqdm,
    )

    # å…³é—­ wandb
    if args.use_wandb:
        wandb.finish()


# ============================================================
# Argument Parser
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")

    # æ–¹æ³•é€‰æ‹©
    parser.add_argument(
        "--method",
        type=str,
        default="simclr",
        choices=["simclr", "moco", "byol", "dino", "dinov2", "ibot", "vicreg", "mae"],
        help="è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•",
    )

    # æ•°æ®ï¼ˆæœ¬åœ° / HuggingFaceï¼‰
    parser.add_argument(
        "--dataset_type",
        type=str,
        default="local",
        choices=["local", "huggingface"],
        help="æ•°æ®æ¥æºï¼šæœ¬åœ°æ–‡ä»¶å¤¹æˆ– HuggingFace æ•°æ®é›†",
    )
    parser.add_argument(
        "--dataset_root",
        type=str,
        default="images/train",
        help="æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼Œä¾‹å¦‚ images/trainï¼ˆdataset_type=local æ—¶ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        help="HuggingFace æ•°æ®é›†åï¼ˆä»…å½“ dataset_type=huggingface æ—¶ä½¿ç”¨ï¼‰",
    )
    parser.add_argument("--img_size", type=int, default=96)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument(
        "--train_sample",
        type=int,
        default=None,
        help="è®­ç»ƒé›†å­é›†å¤§å°ï¼Œä¾‹å¦‚ 50000ï¼ˆNone è¡¨ç¤ºç”¨å…¨éƒ¨ï¼‰",
    )
    parser.add_argument(
        "--aug_strength",
        type=str,
        default="strong",
        choices=["strong", "weak"],
        help="æ•°æ®å¢å¼ºå¼ºåº¦",
    )

    # æ¨¡å‹
    parser.add_argument(
        "--backbone_type",
        type=str,
        default="resnet50",
        choices=["resnet50", "vit_s_16", "vit_b_16", "vit_s_14", "vit_b_14"],
    )
    parser.add_argument(
        "--pretrained_backbone",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒ backbone",
    )

    # æ–¹æ³•ç‰¹å®šå‚æ•°ï¼ˆæ¯”å¦‚ SimCLRï¼‰
    parser.add_argument("--proj_hidden_dim", type=int, default=2048)
    parser.add_argument("--proj_output_dim", type=int, default=128)
    parser.add_argument("--temperature", type=float, default=0.5)

    # è®­ç»ƒ
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument(
        "--optimizer_type",
        type=str,
        default="adamw",
        choices=["adamw", "sgd"],
    )
    parser.add_argument(
        "--scheduler_type",
        type=str,
        default="cosine",
        choices=["cosine", "step"],
    )
    parser.add_argument("--warmup_epochs", type=int, default=0)
    parser.add_argument(
        "--use_amp",
        action="store_true",
        help="ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆä¸åŠ è¯¥å‚æ•°åˆ™ä¸º Falseï¼‰",
    )

    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--exp_name", type=str, default=None, help="å®éªŒåç§°ï¼ˆç”¨äºå‘½åcheckpointç›®å½•ï¼Œä¾‹å¦‚ï¼šdinov2_vitb16_96pxï¼‰")
    parser.add_argument("--save_dir", type=str, default=None, help="ä¿å­˜ç›®å½•ï¼ˆå¦‚æœæä¾›exp_nameï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆï¼š./checkpoints/{exp_name}ï¼‰")
    parser.add_argument("--save_freq", type=int, default=None, help="ä¿å­˜é¢‘ç‡ï¼ˆæ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡ epoch ç‰¹å®šçš„ checkpointï¼Œé»˜è®¤ None è¡¨ç¤ºåªä¿å­˜ latest å’Œ bestï¼‰")
    parser.add_argument("--log_freq", type=int, default=100)
    parser.add_argument("--use_wandb", action="store_true")
    parser.add_argument("--wandb_project", type=str, default="ssl-pretraining")
    parser.add_argument("--wandb_name", type=str, default=None)
    parser.add_argument("--wandb_watch", action="store_true")

    # æ—©åœ
    parser.add_argument(
        "--early_stop_patience",
        type=int,
        default=None,
        help="æ—©åœè€å¿ƒå€¼ï¼ŒNone è¡¨ç¤ºä¸ä½¿ç”¨æ—©åœ",
    )
    parser.add_argument(
        "--early_stop_min_delta",
        type=float,
        default=0.0001,
        help="æ—©åœæœ€å°æ”¹å–„é˜ˆå€¼",
    )
    
    # æ¢å¤è®­ç»ƒ
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="æ¢å¤è®­ç»ƒçš„ checkpoint è·¯å¾„ï¼ˆä¾‹å¦‚ï¼š./checkpoints/exp_name/latest.pth æˆ– ./checkpoints/exp_name/epoch_010.pthï¼‰",
    )

    # è¯„ä¼°å‚æ•°
    parser.add_argument(
        "--eval_enabled",
        action="store_true",
        help="æ˜¯å¦å¯ç”¨è¯„ä¼°ï¼ˆåœ¨ CUB-200-2011 ä¸Šæ¯ N ä¸ª epoch è¯„ä¼°ä¸€æ¬¡ï¼‰",
    )
    parser.add_argument(
        "--eval_cub_data_dir",
        type=str,
        default=None,
        help="CUB-200-2011 æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆåŒ…å« train/val/testï¼Œeval_enabled=True æ—¶å¿…éœ€ï¼‰",
    )
    parser.add_argument(
        "--eval_freq",
        type=int,
        default=2,
        help="è¯„ä¼°é¢‘ç‡ï¼ˆæ¯ N ä¸ª epoch è¯„ä¼°ä¸€æ¬¡ï¼Œé»˜è®¤ 2ï¼‰",
    )
    parser.add_argument(
        "--eval_method",
        type=str,
        default="knn",
        choices=["knn", "linear_probe"],
        help="è¯„ä¼°æ–¹æ³•ï¼šknn æˆ– linear_probe",
    )
    parser.add_argument(
        "--eval_knn_k",
        type=int,
        default=20,
        help="k-NN è¯„ä¼°çš„ k å€¼",
    )
    parser.add_argument(
        "--eval_linear_probe_C",
        type=float,
        default=1.0,
        help="Linear Probe çš„æ­£åˆ™åŒ–å¼ºåº¦",
    )
    parser.add_argument(
        "--eval_use_cls_token",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨ CLS tokenï¼ˆä»… ViTï¼‰",
    )
    parser.add_argument(
        "--eval_batch_size",
        type=int,
        default=256,
        help="è¯„ä¼°æ—¶çš„æ‰¹æ¬¡å¤§å°",
    )
    parser.add_argument(
        "--eval_num_workers",
        type=int,
        default=4,
        help="è¯„ä¼°æ—¶çš„æ•°æ®åŠ è½½çº¿ç¨‹æ•°",
    )

    # å…¶ä»–å‚æ•°
    parser.add_argument(
        "--disable_tqdm",
        action="store_true",
        help="ç¦ç”¨ tqdm è¿›åº¦æ¡ï¼ˆé€‚ç”¨äºéäº¤äº’å¼ç¯å¢ƒæˆ–æ—¥å¿—æ–‡ä»¶ï¼‰",
    )

    return parser.parse_args()


# ============================================================
# Main
# ============================================================

def main():
    torch.backends.cudnn.benchmark = True
    print("=" * 60)
    print("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now()}")
    print("=" * 60)

    args = parse_args()
    main_train(args)

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
