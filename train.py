"""
é€šç”¨è®­ç»ƒå¾ªç¯ - å®Œå…¨å¯ä»¥å…±ç”¨çš„éƒ¨åˆ†
==================================================
ç»Ÿä¸€çš„è®­ç»ƒå¾ªç¯å¤–å£³ï¼Œæ¯ä¸ªæ–¹æ³•åªéœ€è¦å®ç° compute_loss()
"""

import os
import argparse
from datetime import datetime

import torch
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import wandb

from load_data import load_dino_data
from model import build_method
from utils import (
    build_optimizer,
    build_scheduler,
    save_checkpoint,  # ç›®å‰æ²¡ç”¨åˆ°ï¼Œä½†å…ˆç•™ç€
    count_parameters
)


# ============================================================
# é€šç”¨è®­ç»ƒå¾ªç¯
# ============================================================

def train_ssl(
    method,
    train_loader,
    device,
    optimizer,
    scheduler,
    epochs,
    save_dir,
    two_view_aug,
    use_amp=True,
    save_freq=1,
    log_freq=100,
    use_wandb=False,
    wandb_project="ssl-pretraining",
    wandb_name=None,
    early_stop_patience=None,
    early_stop_min_delta=0.0001,
):
    """
    é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒå¾ªç¯
    
    Args:
        method: è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å®ä¾‹ï¼ˆç»§æ‰¿ BaseSSLMethodï¼‰
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•
        two_view_aug: æ•°æ®å¢å¼ºå‡½æ•°ï¼ˆè¾“å…¥ä¸€æ‰¹å›¾åƒï¼Œè¾“å‡ºä¸¤ç»„å¢å¼ºè§†å›¾ï¼‰
        use_amp: æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        save_freq: ä¿å­˜é¢‘ç‡ï¼ˆæ¯ N ä¸ª epochï¼‰
        log_freq: æ—¥å¿—é¢‘ç‡ï¼ˆæ¯ N ä¸ª stepï¼‰
        use_wandb: æ˜¯å¦ä½¿ç”¨ wandb ç›‘æ§
        wandb_project: wandb é¡¹ç›®å
        wandb_name: wandb è¿è¡Œåç§°
        early_stop_patience: æ—©åœè€å¿ƒå€¼ï¼ˆè¿ç»­å¤šå°‘ä¸ªepochæ²¡æœ‰æ”¹å–„åˆ™åœæ­¢ï¼‰ï¼ŒNoneè¡¨ç¤ºä¸ä½¿ç”¨æ—©åœ
        early_stop_min_delta: æ—©åœæœ€å°æ”¹å–„é˜ˆå€¼
    """
    os.makedirs(save_dir, exist_ok=True)

    scaler = GradScaler() if use_amp else None

    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   æ–¹æ³•: {method.__class__.__name__}")
    print(f"   å‚æ•°é‡: {count_parameters(method):,}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   AMP: {use_amp}")
    if use_wandb:
        print(f"   Wandb: âœ… {wandb_project}/{wandb_name}")
    if early_stop_patience is not None:
        print(f"   æ—©åœ: âœ… patience={early_stop_patience}, min_delta={early_stop_min_delta}")
    print()

    global_step = 0
    best_loss = float("inf")
    epochs_without_improvement = 0  # æ—©åœè®¡æ•°å™¨

    # Epoch å¾ªç¯
    for epoch in range(1, epochs + 1):
        method.train()
        epoch_loss = 0.0
        num_batches = 0

        # è¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}", ncols=100)

        for batch in pbar:
            # å°† batch ç§»åˆ°è®¾å¤‡å¹¶åšä¸¤è§†å›¾å¢å¼º
            if isinstance(batch, torch.Tensor):
                batch = batch.to(device, non_blocking=True)
                views = two_view_aug(batch)
            elif isinstance(batch, (list, tuple)):
                batch = [
                    b.to(device, non_blocking=True) if isinstance(b, torch.Tensor) else b
                    for b in batch
                ]
                views = method.get_views(batch)
            else:
                views = method.get_views(batch)

            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
            if use_amp:
                with autocast():
                    loss, loss_dict = method.compute_loss(views)

                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss, loss_dict = method.compute_loss(views)
                loss.backward()
                optimizer.step()

            # æ›´æ–° EMAï¼ˆå¦‚æœæœ‰ teacher ç½‘ç»œï¼‰
            method.update_ema()

            epoch_loss += loss.item()
            num_batches += 1
            global_step += 1

            # Step çº§åˆ«æ—¥å¿—
            if global_step % log_freq == 0:
                current_lr = optimizer.param_groups[0]["lr"]
                pbar.set_postfix({**loss_dict, "lr": f"{current_lr:.2e}"})

                if use_wandb:
                    wandb.log(
                        {
                            "train/loss_step": loss.item(),
                            "train/lr": current_lr,
                            "train/epoch": epoch,
                            **{f"train/{k}": v for k, v in loss_dict.items()},
                        },
                        step=global_step,
                    )

        # Epoch ç»“æŸï¼šæ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            scheduler.step()

        # è®¾ç½® epochï¼ˆç”¨äº DINO/iBOT çš„ warmup ç­‰ï¼‰
        if hasattr(method, "set_epoch"):
            method.set_epoch(epoch)

        avg_loss = epoch_loss / max(1, num_batches)
        current_lr = (
            scheduler.get_last_lr()[0]
            if scheduler is not None
            else optimizer.param_groups[0]["lr"]
        )

        print(f"\nğŸ“Œ Epoch {epoch}/{epochs}:")
        print(f"   avg_loss = {avg_loss:.4f}")
        print(f"   lr = {current_lr:.3e}")

        # Epoch çº§åˆ«æ—¥å¿—
        if use_wandb:
            wandb.log(
                {
                    "train/loss_epoch": avg_loss,
                    "train/lr_epoch": current_lr,
                    "epoch": epoch,
                },
                step=global_step,
            )

        # ä¿å­˜å½“å‰ epoch çš„ checkpoint
        if epoch % save_freq == 0 or epoch == epochs:
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
            }

            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict()
                    if hasattr(scheduler, "scheduler")
                    else scheduler.state_dict()
                )

            save_path = os.path.join(save_dir, f"epoch_{epoch:03d}.pth")
            torch.save(ckpt, save_path)
            print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {save_path}")

            if use_wandb:
                wandb.save(save_path)

        # ä¿å­˜ best æ¨¡å‹ & æ—©åœé€»è¾‘
        if avg_loss < best_loss - early_stop_min_delta:
            best_loss = avg_loss
            epochs_without_improvement = 0  # é‡ç½®æ—©åœè®¡æ•°å™¨

            best_path = os.path.join(save_dir, "best.pth")
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
            }
            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict()
                    if hasattr(scheduler, "scheduler")
                    else scheduler.state_dict()
                )
            torch.save(ckpt, best_path)
            print(f"ğŸ… æ›´æ–° Best æ¨¡å‹ï¼ˆloss={best_loss:.4f}ï¼‰")

            if use_wandb:
                wandb.run.summary["best_loss"] = best_loss
                wandb.run.summary["best_epoch"] = epoch
                wandb.save(best_path)
        else:
            epochs_without_improvement += 1
            if early_stop_patience is not None:
                print(f"âš ï¸  Loss æ²¡æœ‰æ”¹å–„ ({epochs_without_improvement}/{early_stop_patience})")

        # æ—©åœæ£€æŸ¥
        if (
            early_stop_patience is not None
            and epochs_without_improvement >= early_stop_patience
        ):
            print(f"\nğŸ›‘ æ—©åœè§¦å‘ï¼è¿ç»­ {early_stop_patience} ä¸ª epoch æ²¡æœ‰æ”¹å–„")
            print(f"   Best loss: {best_loss:.4f} (Epoch {epoch - early_stop_patience})")
            if use_wandb:
                wandb.run.summary["early_stopped"] = True
                wandb.run.summary["stopped_epoch"] = epoch
            break


# ============================================================
# ä¸»è®­ç»ƒå‡½æ•°
# ============================================================

def main_train(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ è®¾å¤‡: {device}")

    # åˆå§‹åŒ– wandb
    if args.use_wandb:
        wandb_name = (
            args.wandb_name
            or f"{args.method}_{args.backbone_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        wandb.init(
            project=args.wandb_project,
            name=wandb_name,
            config={
                "method": args.method,
                "backbone_type": args.backbone_type,
                "img_size": args.img_size,
                "batch_size": args.batch_size,
                "epochs": args.epochs,
                "lr": args.lr,
                "weight_decay": args.weight_decay,
                "optimizer_type": args.optimizer_type,
                "scheduler_type": args.scheduler_type,
                "warmup_epochs": args.warmup_epochs,
                "temperature": args.temperature,
                "proj_hidden_dim": args.proj_hidden_dim,
                "proj_output_dim": args.proj_output_dim,
                "aug_strength": args.aug_strength,
                "train_sample": args.train_sample,
                "dataset_type": args.dataset_type,
                "dataset_root": args.dataset_root,
                "dataset_name": args.dataset_name,
            },
        )

    # åŠ è½½æ•°æ®ï¼ˆè¿™é‡Œåˆå¹¶æœ¬åœ° / HF å…¥å£ï¼‰
    train_loader, _, _, two_view_aug = load_dino_data(
        dataset_type=args.dataset_type,     # "local" æˆ– "huggingface"
        dataset_root=args.dataset_root,     # æœ¬åœ°æ—¶ä½¿ç”¨
        dataset_name=args.dataset_name,     # HF æ—¶ä½¿ç”¨
        img_size=args.img_size,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        train_sample=args.train_sample,
        strength=args.aug_strength,
    )

    # æ„å»ºæ–¹æ³•é…ç½®
    method_config = {
        "proj_hidden_dim": args.proj_hidden_dim,
        "proj_output_dim": args.proj_output_dim,
        "temperature": args.temperature,
    }

    # æ„å»ºæ–¹æ³•
    method = build_method(
        method_name=args.method,
        backbone_type=args.backbone_type,
        pretrained_backbone=args.pretrained_backbone,
        config=method_config,
    ).to(device)

    # Wandb watch modelï¼ˆå¯é€‰ï¼‰
    if args.use_wandb and args.wandb_watch:
        wandb.watch(method, log="all", log_freq=args.log_freq)

    # æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = build_optimizer(
        method,
        optimizer_type=args.optimizer_type,
        lr=args.lr,
        weight_decay=args.weight_decay,
    )

    scheduler = build_scheduler(
        optimizer,
        scheduler_type=args.scheduler_type,
        T_max=args.epochs,
        warmup_epochs=args.warmup_epochs,
    )

    # è®­ç»ƒ
    train_ssl(
        method=method,
        train_loader=train_loader,
        device=device,
        optimizer=optimizer,
        scheduler=scheduler,
        epochs=args.epochs,
        save_dir=args.save_dir,
        two_view_aug=two_view_aug,
        use_amp=args.use_amp,
        save_freq=args.save_freq,
        log_freq=args.log_freq,
        use_wandb=args.use_wandb,
        wandb_project=args.wandb_project,
        wandb_name=args.wandb_name,
        early_stop_patience=args.early_stop_patience,
        early_stop_min_delta=args.early_stop_min_delta,
    )

    # å…³é—­ wandb
    if args.use_wandb:
        wandb.finish()


# ============================================================
# Argument Parser
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")

    # æ–¹æ³•é€‰æ‹©
    parser.add_argument(
        "--method",
        type=str,
        default="simclr",
        choices=["simclr", "moco", "byol", "dino", "ibot", "vicreg", "mae"],
        help="è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•",
    )

    # æ•°æ®ï¼ˆæœ¬åœ° / HuggingFaceï¼‰
    parser.add_argument(
        "--dataset_type",
        type=str,
        default="local",
        choices=["local", "huggingface"],
        help="æ•°æ®æ¥æºï¼šæœ¬åœ°æ–‡ä»¶å¤¹æˆ– HuggingFace æ•°æ®é›†",
    )
    parser.add_argument(
        "--dataset_root",
        type=str,
        default="images/train",
        help="æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼Œä¾‹å¦‚ images/trainï¼ˆdataset_type=local æ—¶ä½¿ç”¨ï¼‰",
    )
    parser.add_argument(
        "--dataset_name",
        type=str,
        default=None,
        help="HuggingFace æ•°æ®é›†åï¼ˆä»…å½“ dataset_type=huggingface æ—¶ä½¿ç”¨ï¼‰",
    )
    parser.add_argument("--img_size", type=int, default=96)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument(
        "--train_sample",
        type=int,
        default=None,
        help="è®­ç»ƒé›†å­é›†å¤§å°ï¼Œä¾‹å¦‚ 50000ï¼ˆNone è¡¨ç¤ºç”¨å…¨éƒ¨ï¼‰",
    )
    parser.add_argument(
        "--aug_strength",
        type=str,
        default="strong",
        choices=["strong", "weak"],
        help="æ•°æ®å¢å¼ºå¼ºåº¦",
    )

    # æ¨¡å‹
    parser.add_argument(
        "--backbone_type",
        type=str,
        default="resnet50",
        choices=["resnet50", "vit_b_16"],
    )
    parser.add_argument(
        "--pretrained_backbone",
        action="store_true",
        help="æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒ backbone",
    )

    # æ–¹æ³•ç‰¹å®šå‚æ•°ï¼ˆæ¯”å¦‚ SimCLRï¼‰
    parser.add_argument("--proj_hidden_dim", type=int, default=2048)
    parser.add_argument("--proj_output_dim", type=int, default=128)
    parser.add_argument("--temperature", type=float, default=0.5)

    # è®­ç»ƒ
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument(
        "--optimizer_type",
        type=str,
        default="adamw",
        choices=["adamw", "sgd"],
    )
    parser.add_argument(
        "--scheduler_type",
        type=str,
        default="cosine",
        choices=["cosine", "step"],
    )
    parser.add_argument("--warmup_epochs", type=int, default=0)
    parser.add_argument(
        "--use_amp",
        action="store_true",
        help="ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆä¸åŠ è¯¥å‚æ•°åˆ™ä¸º Falseï¼‰",
    )

    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--save_dir", type=str, default="./checkpoints")
    parser.add_argument("--save_freq", type=int, default=1)
    parser.add_argument("--log_freq", type=int, default=100)
    parser.add_argument("--use_wandb", action="store_true")
    parser.add_argument("--wandb_project", type=str, default="ssl-pretraining")
    parser.add_argument("--wandb_name", type=str, default=None)
    parser.add_argument("--wandb_watch", action="store_true")

    # æ—©åœ
    parser.add_argument(
        "--early_stop_patience",
        type=int,
        default=None,
        help="æ—©åœè€å¿ƒå€¼ï¼ŒNone è¡¨ç¤ºä¸ä½¿ç”¨æ—©åœ",
    )
    parser.add_argument(
        "--early_stop_min_delta",
        type=float,
        default=0.0001,
        help="æ—©åœæœ€å°æ”¹å–„é˜ˆå€¼",
    )

    return parser.parse_args()


# ============================================================
# Main
# ============================================================

def main():
    torch.backends.cudnn.benchmark = True
    print("=" * 60)
    print("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now()}")
    print("=" * 60)

    args = parse_args()
    main_train(args)

    print("\nâœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
