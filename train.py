"""
é€šç”¨è®­ç»ƒå¾ªç¯ - å®Œå…¨å¯ä»¥å…±ç”¨çš„éƒ¨åˆ†
==================================================
ç»Ÿä¸€çš„è®­ç»ƒå¾ªç¯å¤–å£³ï¼Œæ¯ä¸ªæ–¹æ³•åªéœ€è¦å®ç° compute_loss()
"""

import os
import argparse
from datetime import datetime

import torch
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import wandb 
from load_data import load_dino_data
from model import build_method
from utils import (
    build_optimizer,
    build_scheduler,
    save_checkpoint,
    count_parameters
)


# ============================================================
# é€šç”¨è®­ç»ƒå¾ªç¯
# ============================================================

def train_ssl(
    method,
    train_loader,
    device,
    optimizer,
    scheduler,
    epochs,
    save_dir,
    two_view_aug,
    use_amp=True,
    save_freq=1,
    log_freq=100,
    use_wandb=False,  # â¬…ï¸ æ·»åŠ è¿™ä¸ªå‚æ•°
    wandb_project="ssl-pretraining",  # â¬…ï¸ æ·»åŠ è¿™ä¸ªå‚æ•°
    wandb_name=None  # â¬…ï¸ æ·»åŠ è¿™ä¸ªå‚æ•°
):
    """
    é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒå¾ªç¯
    
    Args:
        method: è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•å®ä¾‹ï¼ˆç»§æ‰¿ BaseSSLMethodï¼‰
        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        device: è®¾å¤‡
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
        epochs: è®­ç»ƒè½®æ•°
        save_dir: ä¿å­˜ç›®å½•
        two_view_aug: æ•°æ®å¢å¼ºå‡½æ•°
        use_amp: æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦
        save_freq: ä¿å­˜é¢‘ç‡ï¼ˆæ¯ N ä¸ª epochï¼‰
        log_freq: æ—¥å¿—é¢‘ç‡ï¼ˆæ¯ N ä¸ª stepï¼‰
        use_wandb: æ˜¯å¦ä½¿ç”¨ wandb ç›‘æ§
        wandb_project: wandb é¡¹ç›®å
        wandb_name: wandb è¿è¡Œåç§°
    """
    os.makedirs(save_dir, exist_ok=True)
    
    scaler = GradScaler() if use_amp else None
    
    print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"   æ–¹æ³•: {method.__class__.__name__}")
    print(f"   å‚æ•°é‡: {count_parameters(method):,}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   AMP: {use_amp}")
    if use_wandb:
        print(f"   Wandb: âœ… {wandb_project}/{wandb_name}")
    print()
    
    global_step = 0
    best_loss = float("inf")
    
    # Epoch å¾ªç¯
    for epoch in range(1, epochs + 1):
        method.train()
        epoch_loss = 0
        num_batches = 0
        
        # è¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs}", ncols=100)
        
        for batch in pbar:
            # å°† batch ç§»åˆ°è®¾å¤‡å¹¶åšå¢å¼º
            if isinstance(batch, torch.Tensor):
                batch = batch.to(device, non_blocking=True)
                views = two_view_aug(batch)
            elif isinstance(batch, (list, tuple)):
                batch = [b.to(device, non_blocking=True) if isinstance(b, torch.Tensor) else b for b in batch]
                views = method.get_views(batch)
            else:
                views = method.get_views(batch)
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­å’ŒæŸå¤±è®¡ç®—
            if use_amp:
                with autocast():
                    loss, loss_dict = method.compute_loss(views)
                
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss, loss_dict = method.compute_loss(views)
                loss.backward()
                optimizer.step()
            
            # æ›´æ–° EMAï¼ˆå¦‚æœæœ‰ teacher ç½‘ç»œï¼‰
            method.update_ema()
            
            epoch_loss += loss.item()
            num_batches += 1
            global_step += 1
            
            # æ—¥å¿—
            if global_step % log_freq == 0:
                current_lr = optimizer.param_groups[0]['lr']
                pbar.set_postfix({**loss_dict, "lr": f"{current_lr:.2e}"})
                
                # â¬‡ï¸ Wandb æ—¥å¿—ï¼ˆstepçº§åˆ«ï¼‰
                if use_wandb:
                    wandb.log({
                        "train/loss_step": loss.item(),
                        "train/lr": current_lr,
                        "train/epoch": epoch,
                        **{f"train/{k}": v for k, v in loss_dict.items()}
                    }, step=global_step)
        
        # Epoch ç»“æŸï¼šæ›´æ–°å­¦ä¹ ç‡
        if scheduler is not None:
            scheduler.step()
        
        # è®¾ç½® epochï¼ˆç”¨äº DINO/iBOT çš„ warmupï¼‰
        if hasattr(method, 'set_epoch'):
            method.set_epoch(epoch)
        
        avg_loss = epoch_loss / max(1, num_batches)
        current_lr = scheduler.get_last_lr()[0] if scheduler is not None else optimizer.param_groups[0]['lr']
        
        print(f"\nğŸ“Œ Epoch {epoch}/{epochs}:")
        print(f"   avg_loss = {avg_loss:.4f}")
        print(f"   lr = {current_lr:.3e}")
        
        # â¬‡ï¸ Wandb æ—¥å¿—ï¼ˆepochçº§åˆ«ï¼‰
        if use_wandb:
            wandb.log({
                "train/loss_epoch": avg_loss,
                "train/lr_epoch": current_lr,
                "epoch": epoch
            }, step=global_step)
        
        # ä¿å­˜ checkpoint
        if epoch % save_freq == 0 or epoch == epochs:
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
            }
            
            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict() 
                    if hasattr(scheduler, 'scheduler') 
                    else scheduler.state_dict()
                )
            
            save_path = os.path.join(save_dir, f"epoch_{epoch:03d}.pth")
            torch.save(ckpt, save_path)
            print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹åˆ° {save_path}")
            
            # â¬‡ï¸ Wandb ä¿å­˜ checkpoint
            if use_wandb:
                wandb.save(save_path)
        
        # ä¿å­˜ best æ¨¡å‹
        if avg_loss < best_loss:
            best_loss = avg_loss
            best_path = os.path.join(save_dir, "best.pth")
            ckpt = {
                "epoch": epoch,
                "model_state_dict": method.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "avg_loss": avg_loss,
                "global_step": global_step,
            }
            if scheduler is not None:
                ckpt["scheduler_state_dict"] = (
                    scheduler.scheduler.state_dict() 
                    if hasattr(scheduler, 'scheduler') 
                    else scheduler.state_dict()
                )
            torch.save(ckpt, best_path)
            print(f"ğŸ… æ›´æ–° Best æ¨¡å‹ï¼ˆloss={best_loss:.4f}ï¼‰")
            
            # â¬‡ï¸ Wandb è®°å½• best loss
            if use_wandb:
                wandb.run.summary["best_loss"] = best_loss
                wandb.run.summary["best_epoch"] = epoch
                wandb.save(best_path)

# ============================================================
# ä¸»è®­ç»ƒå‡½æ•°
# ============================================================

def main_train(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ è®¾å¤‡: {device}")
    
    # â¬‡ï¸ åˆå§‹åŒ– wandb
    if args.use_wandb:
        wandb_name = args.wandb_name or f"{args.method}_{args.backbone_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        wandb.init(
            project=args.wandb_project,
            name=wandb_name,
            config={
                "method": args.method,
                "backbone_type": args.backbone_type,
                "img_size": args.img_size,
                "batch_size": args.batch_size,
                "epochs": args.epochs,
                "lr": args.lr,
                "weight_decay": args.weight_decay,
                "optimizer_type": args.optimizer_type,
                "scheduler_type": args.scheduler_type,
                "warmup_epochs": args.warmup_epochs,
                "temperature": args.temperature,
                "proj_hidden_dim": args.proj_hidden_dim,
                "proj_output_dim": args.proj_output_dim,
                "aug_strength": args.aug_strength,
                "train_sample": args.train_sample,
            }
        )
    
    # åŠ è½½æ•°æ®
    train_loader, _, _, two_view_aug = load_dino_data(
        dataset_name=args.dataset_name,
        dataset_type=args.dataset_type,
        img_size=args.img_size,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        train_sample=args.train_sample,
        eval_samples=None,
        strength=args.aug_strength,
    )
    
    # æ„å»ºæ–¹æ³•é…ç½®
    method_config = {
        "proj_hidden_dim": args.proj_hidden_dim,
        "proj_output_dim": args.proj_output_dim,
        "temperature": args.temperature,
    }
    
    # æ„å»ºæ–¹æ³•
    method = build_method(
        method_name=args.method,
        backbone_type=args.backbone_type,
        pretrained_backbone=args.pretrained_backbone,
        config=method_config
    ).to(device)
    
    # â¬‡ï¸ Wandb watch modelï¼ˆå¯é€‰ï¼Œè®°å½•æ¢¯åº¦å’Œå‚æ•°ï¼‰
    if args.use_wandb and args.wandb_watch:
        wandb.watch(method, log="all", log_freq=args.log_freq)
    
    # æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = build_optimizer(
        method,
        optimizer_type=args.optimizer_type,
        lr=args.lr,
        weight_decay=args.weight_decay
    )
    
    scheduler = build_scheduler(
        optimizer,
        scheduler_type=args.scheduler_type,
        T_max=args.epochs,
        warmup_epochs=args.warmup_epochs
    )
    
    # è®­ç»ƒ
    train_ssl(
        method=method,
        train_loader=train_loader,
        device=device,
        optimizer=optimizer,
        scheduler=scheduler,
        epochs=args.epochs,
        save_dir=args.save_dir,
        two_view_aug=two_view_aug,
        use_amp=args.use_amp,
        save_freq=args.save_freq,
        log_freq=args.log_freq,
        use_wandb=args.use_wandb,  # â¬…ï¸ ä¼ å…¥ wandb å‚æ•°
        wandb_project=args.wandb_project,
        wandb_name=args.wandb_name
    )
    
    # â¬‡ï¸ å…³é—­ wandb
    if args.use_wandb:
        wandb.finish()


# ============================================================
# Argument Parser
# ============================================================

def parse_args():
    parser = argparse.ArgumentParser("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")
    
    # æ–¹æ³•é€‰æ‹©
    parser.add_argument("--method", type=str, default="simclr",
                       choices=["simclr", "moco", "byol", "dino", "ibot", "vicreg", "mae"],
                       help="è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•")
    
    # æ•°æ®
    parser.add_argument("--dataset_type", type=str, default="huggingface")
    parser.add_argument("--dataset_name", type=str, default="tsbpp/fall2025_deeplearning")
    parser.add_argument("--img_size", type=int, default=96)
    parser.add_argument("--num_workers", type=int, default=8)
    parser.add_argument("--train_sample", type=int, default=None,
                       help="è®­ç»ƒé›†å­é›†å¤§å°ï¼Œä¾‹å¦‚ 50000")
    parser.add_argument("--aug_strength", type=str, default="strong",
                       choices=["strong", "weak"])
    
    # æ¨¡å‹
    parser.add_argument("--backbone_type", type=str, default="resnet50",
                       choices=["resnet50", "vit_b_16"])
    parser.add_argument("--pretrained_backbone", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒ backbone")
    
    # æ–¹æ³•ç‰¹å®šå‚æ•°ï¼ˆSimCLRï¼‰
    parser.add_argument("--proj_hidden_dim", type=int, default=2048)
    parser.add_argument("--proj_output_dim", type=int, default=128)
    parser.add_argument("--temperature", type=float, default=0.5)
    
    # è®­ç»ƒ
    parser.add_argument("--batch_size", type=int, default=128)
    parser.add_argument("--epochs", type=int, default=100)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--weight_decay", type=float, default=1e-4)
    parser.add_argument("--optimizer_type", type=str, default="adamw",
                       choices=["adamw", "sgd"])
    parser.add_argument("--scheduler_type", type=str, default="cosine",
                       choices=["cosine", "step"])
    parser.add_argument("--warmup_epochs", type=int, default=0)
    parser.add_argument("--use_amp", action="store_true", default=True,
                       help="ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦")
    
    # ä¿å­˜å’Œæ—¥å¿—
    parser.add_argument("--save_dir", type=str, default="./checkpoints")
    parser.add_argument("--save_freq", type=int, default=1,
                       help="æ¯ N ä¸ª epoch ä¿å­˜ä¸€æ¬¡")
    parser.add_argument("--log_freq", type=int, default=100,
                       help="æ¯ N ä¸ª step è®°å½•ä¸€æ¬¡æ—¥å¿—")
    parser.add_argument("--use_wandb", action="store_true",
                       help="ä½¿ç”¨ Weights & Biases ç›‘æ§")
    parser.add_argument("--wandb_project", type=str, default="ssl-pretraining",
                       help="Wandb é¡¹ç›®åç§°")
    parser.add_argument("--wandb_name", type=str, default=None,
                       help="Wandb è¿è¡Œåç§°ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰")
    parser.add_argument("--wandb_watch", action="store_true",
                       help="Wandb watch æ¨¡å‹ï¼ˆè®°å½•æ¢¯åº¦ï¼Œä¼šå˜æ…¢ï¼‰")
    return parser.parse_args()


# ============================================================
# Main
# ============================================================

def main():
    torch.backends.cudnn.benchmark = True
    print("=" * 60)
    print("é€šç”¨è‡ªç›‘ç£å­¦ä¹ è®­ç»ƒæ¡†æ¶")
    print(f"å¯åŠ¨æ—¶é—´: {datetime.now()}")
    print("=" * 60)
    
    args = parse_args()
    main_train(args)
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()
